
% approximate-inference.tex

\documentclass[dvipdfmx,notheorems,t]{beamer}

\usepackage{docmute}
\input{settings}

\begin{document}

\section{近似推論法}

\subsection{変分推論}

\begin{frame}{変分推論}

\begin{itemize}
	\item 変分推論が必要だった理由
	\begin{itemize}
		\item 潜在変数に関する事後分布$p(\bm{Z} | \bm{X}, \bm{\theta})$の計算は、困難であることが多い
		\item どのような場合に困難になるのか、次の図\ref{fig:example-of-inference-problems}に示す
		\newline
		\item $p(\bm{Z} | \bm{X}, \bm{\theta})$の厳密な計算は諦める代わりに、\alert{別の確率分布で近似}したい
		\newline
		\item 別の確率分布で近似するとき、単純な項の積として表現できるといった、\alert{何らかの仮定を置く}
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{変分推論}

\begin{figure}[h]
	\centering
	\includegraphics[clip,scale=0.6,trim=2cm 17.8cm 2cm 5cm,page=647]{../goodfellow-deep-learning.pdf}
	\caption{事後分布$p(\bm{Z} | \bm{X})$の計算が困難な場合}
	\label{fig:example-of-inference-problems}
\end{figure}

\end{frame}

\begin{frame}{変分推論}

\begin{figure}[h]
	\centering
	\includegraphics[clip,scale=0.6,trim=2cm 10cm 2cm 11.8cm,page=647]{../goodfellow-deep-learning.pdf}
	\caption{事後分布$p(\bm{Z} | \bm{X})$の計算が困難な場合}
	\label{fig:example-of-inference-problems-2}
\end{figure}

\end{frame}

\begin{frame}{変分推論}

\begin{itemize}
	\item 事後分布$p(\bm{Z} | \bm{X})$の計算が困難な場合1
	\begin{itemize}
		\item グラフィカルモデルにおいて、\alert{潜在変数間の相互作用がある}場合、計算が困難になる
		\newline
		\item 左側の\alert{半制限付きボルツマンマシン}では、全ての潜在変数の組み合わせ間で、接続がある
		\item 従って、潜在変数間に\alert{依存関係}が存在し、事後分布の計算が手に負えない
		\newline
		\item 白丸で描かれた潜在変数を$\bm{Z} = \left\{ z_1, z_2, z_3 \right\}$、灰色で描かれた観測データを$\bm{X}$とおくと、事後分布$p(\bm{Z} | \bm{X})$は、例えば次のようになる
		\begin{eqnarray}
			&& p(z_1, z_2, z_3 | \bm{X}) \nonumber \\
			&=& p(z_1 | \bm{X}, z_2, z_3) p(z_2 | \bm{X}, z_3) p(z_3 | \bm{X})
		\end{eqnarray}
	\end{itemize} \
	
	\item 事後分布$p(\bm{Z} | \bm{X})$の計算が困難な場合2
	\begin{itemize}
		\item 中央は、層間の結合がない潜在変数の層で構成される、\alert{深層ボルツマンマシン}を表す
		\item 潜在変数の層間の結合があるため、事後分布の計算が手に負えない
		\newline
		\item 上の層の潜在変数を$\bm{Z}_1 = \left\{ z_{11}, z_{12}, z_{13} \right\}$、中間層の潜在変数を$\bm{Z}_2 = \left\{ z_{21}, z_{22}, z_{23} \right\}$、灰色で描かれた観測データを$\bm{X}$とおくと、事後分布は、例えば次のようになる
		\begin{eqnarray}
			&& p(\bm{Z}_1, \bm{Z}_2 | \bm{X}) \nonumber \\
			&=& p(\bm{Z}_2 | \bm{X}) p(\bm{Z}_1 | \bm{Z}_2) \\
			&=& p(z_{21}, z_{22}, z_{23} | \bm{X}) p(z_{11}, z_{12}, z_{13} | \bm{Z}_2) \nonumber \\
			&=& p(z_{21} | \bm{X}) p(z_{22} | \bm{X}) p(z_{23} | \bm{X}) \nonumber \\
			&& \qquad p(z_{11} | \bm{Z}_2) p(z_{12} | \bm{Z}_2) p(z_{13} | \bm{Z}_2)
		\end{eqnarray}
	\end{itemize}
\end{itemize}

\end{frame}

\subsection{座標降下法}

\begin{frame}{変分推論}

\begin{itemize}
	\item 変分推論の目的
	\begin{itemize}
		\item 同時分布$p(\bm{X}, \bm{Z})$が求まっているときに、\alert{事後分布}$p(\bm{Z} | \bm{X})$と、\alert{エビデンス}$p(\bm{X})$の近似を求める
	\end{itemize} \
	
	\item 注意点
	\begin{itemize}
		\item 観測変数と潜在変数をそれぞれ、$\bm{X}, \bm{Z}$とおく
		\item $p(\bm{X})$は、確率モデルからデータ$\bm{X}$が生起する確率である
		\item \alert{データからみたモデルの好み}と解釈できるから、$p(\bm{X})$を\alert{モデルエビデンス}という
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{変分推論}

\begin{itemize}
	\item 周辺分布の対数$\ln p(\bm{X})$の分解
	\begin{itemize}
		\item EMアルゴリズムのときと同様であり、次のように分解できる
		\begin{equation}
			\ln p(\bm{X}) = \mathcal{L}(q) + \KL (q || p)
		\end{equation}
		
		\item 但し、$\mathcal{L}(q)$と$\KL (q || p)$は次のように定義した
		\begin{eqnarray}
			\mathcal{L}(q) &=& \int q(\bm{Z}) \ln \frac{p(\bm{X}, \bm{Z})}{q(\bm{Z})} d\bm{Z} \\
			\KL (q || p) &=& -\int q(\bm{Z}) \ln \frac{p(\bm{Z} | \bm{X})}{q(\bm{Z})} d\bm{Z}
		\end{eqnarray}
	\end{itemize} \
		
	\item パラメータ$\bm{\theta}$の扱い
	\begin{itemize}
		\item EMアルゴリズムとは異なり、\color{red}パラメータ$\bm{\theta}$がどこにも現れていない\normalcolor
		% \item ここでは、全てのパラメータに事前分布が与えられた、\alert{完全にベイズ的なモデル}を考えている
		% \item パラメータも、確率変数として$\bm{Z}$の中に含まれている
		\item \alert{パラメータも潜在変数として扱っている}ので、パラメータベクトルは明示的には書かない
		\item ここでは、パラメータ$\bm{\theta}$については、あまり気にしない
		\newline
		\item ここでは連続潜在変数について考えるが、離散潜在変数であれば、積分を$\bm{Z}$に関する総和に置き換えればよい
	\end{itemize} \
	
	\item 下界$\mathcal{L}(q)$を最適化する動機
	\begin{itemize}
		\item $\mathcal{L}(q)$はエビデンスの対数$\ln p(\bm{X})$の下界であるから、\alert{エビデンス下界}(Evidence lower bound, \alert{ELBO})ともいう
		\item または、負の\alert{変分自由エネルギー}(Variational free energy)という
		\newline
		
		\item $\ln p(\bm{X})$は$q$には依存しないため、定数項とみなせる
		\item 従って、$\mathcal{L}(q)$を$q$について最大化することは、$\KL (q || p)$の最小化に相当
		\item このとき、分布$q(\bm{Z})$を真の事後分布$p(\bm{Z} | \bm{X})$に近づけられる
		\item $q(\bm{Z}) = p(\bm{Z} | \bm{X})$が分かれば、データ$\bm{X}$から、\color{red}潜在変数やパラメータ$\bm{Z}$が得られる\normalcolor
	\end{itemize} \
	
	\framebreak
	
	\item 下界$\mathcal{L}(q)$の最適化
	\begin{itemize}
		\item EMアルゴリズムのときと同じように、下界$\mathcal{L}(q)$を、分布$q(\bm{Z})$について最大化する
		\item これは、KLダイバージェンス$\KL (q || p)$を最小化することと等価である
		\newline
		\item 従って、\color{red}もし$q(\bm{Z})$を任意の分布にしてよければ\normalcolor 、$q(\bm{Z}) = p(\bm{Z} | \bm{X})$とおいて、KLダイバージェンスを$0$にすればよい
		\newline
		\item しかしここでは、\color{red}真の事後分布$p(\bm{Z} | \bm{X})$を求めることは不可能\normalcolor と仮定する
	\end{itemize} \
	
	\item 分布$q(\bm{Z})$の近似
	\begin{itemize}
		\item 計算コストを削減するために、$q(\bm{Z})$の形をある程度\alert{制限する}
		\item 制限したクラスの$q(\bm{Z})$の中で、KLダイバージェンス$\KL (q || p)$を最小化するものを探す
	\end{itemize} \
	
	\item 変分推論の目的
	\begin{itemize}
		\item 分布のクラスを制限することで、$q(\bm{Z})$を計算可能にすること
		\item 表現力が豊かなクラスを使うことで、真の事後分布$p(\bm{Z} | \bm{X})$を良く近似する
		\newline
		\item 計算可能な分布のクラスの中で、\alert{可能な限り豊かな表現力を持つ}ものを選びたい
		\item 表現力が豊かな分布を使うことは、真の事後分布を、精度良く近似することにつながるのであって、従って\alert{過学習は発生しない}
		\newline
		\item 分布$q(\bm{Z})$は、$q(\bm{Z} | \bm{X})$と書くこともある
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{変分推論}

\begin{itemize}
	\item 分布$q(\bm{Z})$のクラスを制限する方法
	\begin{itemize}
		\item 例えば分布$q(\bm{Z})$を、\alert{パラメトリックな分布に限定}することができる
		\item 即ち、パラメータベクトル$\bm{\omega}$によって$q(\bm{Z} | \bm{\omega})$と記述されるような、分布に制限する
		\newline
		\item 分布$q(\bm{Z})$を、ガウス分布などの、何らかの特別なパラメトリックな分布と仮定することに相当
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{変分推論}

\begin{itemize}
	\item クラスを制限する別の方法(\alert{平均場近似})
	\begin{itemize}
		\item 分布$q(\bm{Z})$のクラスを制限する別の方法として、\alert{平均場近似}がある
		\newline
		\item 潜在変数$\bm{Z}$を、$M$個の\color{red}互いに排反なグループ$\left\{ \bm{Z}_1, \ldots, \bm{Z}_M \right\}$に分割\normalcolor
		\item 分布$q(\bm{Z})$が、\alert{これらのグループによって分解されると仮定}
		\begin{equation}
			q(\bm{Z}) = \prod_{i = 1}^M q_i(\bm{Z}_i)
		\end{equation}
		
		\item 分布$q$について、\alert{これ以上の仮定はしない}
		\item 従って、各因子$q_i(\bm{Z}_i)$の関数形については、\alert{何の制限も課さない}
		\newline
		\item 平均場近似とは、元々は物理学における用語である
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{変分推論}

\begin{itemize}
	\item 下界$\mathcal{L}(q)$の最大化
	\begin{itemize}
		\item $q(\bm{Z}) = \prod_i q_i(\bm{Z}_i)$と分解できるような分布$q(\bm{Z})$の中で、\color{red}下界$\mathcal{L}(q)$を最大にするもの\normalcolor を探す
		\newline
		\item $\mathcal{L}(q)$を$q(\bm{Z})$について最大化するために、$\mathcal{L}(q)$を各因子$q_i(\bm{Z}_i)$について\color{red}順番に最大化\normalcolor していく
		\item $\mathcal{L}(q)$に$q(\bm{Z}) = \prod_i q_i(\bm{Z}_i)$を代入して、因子の一つ$q_j(\bm{Z}_j)$に関する\alert{依存項}を抜き出してみよう
		\begin{eqnarray}
			\mathcal{L}(q) &=& \int q(\bm{Z}) \ln \frac{p(\bm{X}, \bm{Z})}{q(\bm{Z})} d\bm{Z} \\
			&=& \int \left( \prod_i q_i(\bm{Z}_i) \right) \ln \frac{p(\bm{X}, \bm{Z})}{q(\bm{Z})} d\bm{Z} \\
			&=& \int \prod_i q_i(\bm{Z}_i) \left( \ln p(\bm{X}, \bm{Z}) - \sum_i \ln q_i(\bm{Z}_i) \right) d\bm{Z} \\
			&=& \int \prod_i q_i(\bm{Z}_i) \left( \ln p(\bm{X}, \bm{Z}) \right) d\bm{Z} - \nonumber \\
			&& \qquad \int \prod_i q_i(\bm{Z}_i) \left( \sum_i \ln q_i(\bm{Z}_i) \right) d\bm{Z}
		\end{eqnarray}
		ここで第1項は
		\begin{eqnarray}
			&& \int \prod_i q_i(\bm{Z}_i) \left( \ln p(\bm{X}, \bm{Z}) \right) d\bm{Z} \nonumber \\
			&=& \int q_j(\bm{Z}_j) \left( \prod_{i \neq j} q_i(\bm{Z}_i) \right) \left( \ln p(\bm{X}, \bm{Z}) \right) d\bm{Z}
		\end{eqnarray}
		$d\bm{Z} = d\bm{Z}_1 d\bm{Z}_2 \cdots d\bm{Z}_M$であるから
		\begin{eqnarray}
			&=& \int q_j(\bm{Z}_j) \left( \prod_{i \neq j} q_i(\bm{Z}_i) \right) \left( \ln p(\bm{X}, \bm{Z}) \right) d\bm{Z}_1 d\bm{Z}_2 \cdots d\bm{Z}_M \\
			&=& \int q_j(\bm{Z}_j) \left( \ln p(\bm{X}, \bm{Z}) \right) \left( \prod_{i \neq j} q_i(\bm{Z}_i) \right) \left( \prod_{i \neq j} d\bm{Z}_i \right) d\bm{Z}_j \\
			&=& \int q_j(\bm{Z}_j) \left( \int \left( \ln p(\bm{X}, \bm{Z}) \right) \prod_{i \neq j} q_i(\bm{Z}_i) d\bm{Z}_i \right) d\bm{Z}_j \\
			&=& \int q_j(\bm{Z}_j) \ln \widetilde{p}(\bm{X}, \bm{Z}_j) d\bm{Z}_j
		\end{eqnarray}
		
		\item 但し、新しい分布$\widetilde{p}(\bm{X}, \bm{Z}_j)$は以下の式で定義した(積分の結果であるため、定数項が出現する)
		\begin{equation}
			\ln \widetilde{p}(\bm{X}, \bm{Z}_j) = \mathbb{E}_{i \neq j}[\ln p(\bm{X}, \bm{Z})] + \mathrm{Const.}
		\end{equation}
		
		\item 記法$\mathbb{E}_{i \neq j}$は、$i \neq j$をみたす全ての分布$q_i(\bm{Z}_i)$による、期待値を取ることを表す
		\begin{equation}
			\mathbb{E}_{i \neq j}[\ln p(\bm{X}, \bm{Z})] = \int \left( \ln p(\bm{X}, \bm{Z}) \right) \prod_{i \neq j} q_i(\bm{Z}_i) d\bm{Z}_i
		\end{equation}
		
		\item 第2項は
		\begin{eqnarray}
			&& \int \prod_i q_i(\bm{Z}_i) \left( \sum_i \ln q_i(\bm{Z}_i) \right) d\bm{Z} \nonumber \\
			&=& \sum_i \int \prod_i q_i(\bm{Z}_i) \left( \ln q_i(\bm{Z}_i) \right) d\bm{Z} \\
			&=& \sum_i \int \prod_k q_k(\bm{Z}_k) \left( \ln q_i(\bm{Z}_i) \right) d\bm{Z}_1 d\bm{Z}_2 \cdots d\bm{Z}_M \\
			&=& \int \prod_k q_k(\bm{Z}_k) \left( \ln q_j(\bm{Z}_j) \right) d\bm{Z}_1 d\bm{Z}_2 \cdots d\bm{Z}_M + \nonumber \\
			&& \qquad \sum_{i \neq j} \int \prod_k q_k(\bm{Z}_k) \left( \ln q_i(\bm{Z}_i) \right) d\bm{Z}_1 d\bm{Z}_2 \cdots d\bm{Z}_M
		\end{eqnarray}
		但し
		\begin{eqnarray}
			&& \int \prod_k q_k(\bm{Z}_k) \left( \ln q_j(\bm{Z}_j) \right) d\bm{Z}_1 d\bm{Z}_2 \cdots d\bm{Z}_M \\
			&=& \int q_j(\bm{Z}_j) \ln q_j(\bm{Z}_j) \left( \prod_{k \neq j} q_k(\bm{Z}_k) \right) \left( \prod_{k \neq j} d\bm{Z}_k \right) d\bm{Z}_j \\
			&=& \int q_j(\bm{Z}_j) \ln q_j(\bm{Z}_j) \left( \int \prod_{k \neq j} q_k(\bm{Z}_k) d\bm{Z}_k \right) d\bm{Z}_j \\
			&=& \int q_j(\bm{Z}_j) \ln q_j(\bm{Z}_j) \prod_{k \neq j} \underbrace{\left( \int q_k(\bm{Z}_k) d\bm{Z}_k \right)}_{=1} d\bm{Z}_j \\
			&=& \int q_j(\bm{Z}_j) \ln q_j(\bm{Z}_j) d\bm{Z}_j
		\end{eqnarray}
		であるほか
		\begin{eqnarray}
			&& \sum_{i \neq j} \int \prod_k q_k(\bm{Z}_k) \left( \ln q_i(\bm{Z}_i) \right) d\bm{Z}_1 d\bm{Z}_2 \cdots d\bm{Z}_M \nonumber \\
			&=& \sum_{i \neq j} \int q_i(\bm{Z}_i) q_j(\bm{Z}_j) \left( \prod_{k \neq i, j} \ln q_k(\bm{Z}_k) \right) \nonumber \\
			&& \qquad \left( \ln q_i(\bm{Z}_i) \right) \left( \prod_{k \neq i, j} d\bm{Z}_k \right) d\bm{Z}_i d\bm{Z}_j \\
			&=& \sum_{i \neq j} \underbrace{\left( \int q_j(\bm{Z}_j) d\bm{Z}_j \right)}_{=1} \left( \int \prod_{k \neq i, j} \ln q_k(\bm{Z}_k) d\bm{Z}_k \right) \nonumber \\
			&& \qquad \int \ln q_i(\bm{Z}_i) q_i(\bm{Z}_i) d\bm{Z}_i \\
			&=& \sum_{i \neq j} \prod_{k \neq i, j} \underbrace{\left( \int \ln q_k(\bm{Z}_k) d\bm{Z}_k \right)}_{=\mathrm{Const.}} \underbrace{\int \ln q_i(\bm{Z}_i) q_i(\bm{Z}_i) d\bm{Z}_i}_{=\mathrm{Const.}} \\
			&=& \sum_{i \neq j} \mathrm{Const.} = \mathrm{Const.}
		\end{eqnarray}
		となるから結局
		\begin{eqnarray}
			&& \int \prod_i q_i(\bm{Z}_i) \left( \sum_i \ln q_i(\bm{Z}_i) \right) d\bm{Z} \nonumber \\
			&=& \int q_j(\bm{Z}_j) \ln q_j(\bm{Z}_j) d\bm{Z}_j + \mathrm{Const.}
		\end{eqnarray}
		
		\item 従って、下界$\mathcal{L}(q)$から$q_j(\bm{Z}_j)$に依存する項を取り出すと
		\begin{eqnarray}
			&& \mathcal{L}(q) = \int q_j(\bm{Z}_j) \ln \widetilde{p}(\bm{X}, \bm{Z}_j) d\bm{Z}_j - \nonumber \\
			&& \qquad \int q_j(\bm{Z}_j) \ln q_j(\bm{Z}_j) d\bm{Z}_j + \mathrm{Const.}
		\end{eqnarray}
		但し
		\begin{equation}
			\ln \widetilde{p}(\bm{X}, \bm{Z}_j) = \mathbb{E}_{i \neq j}[\ln p(\bm{X}, \bm{Z})] = \int \ln p(\bm{X}, \bm{Z}) \prod_{i \neq j} q_i(\bm{Z}_i) d\bm{Z}_i
		\end{equation}
		
		\item $\mathcal{L}(q)$を、$i \neq j$である全ての$q_i(\bm{Z}_i)$について\alert{固定}した上で、$q_j(\bm{Z}_j)$について最大化することになる
		\item $q_j(\bm{Z}_j)$について可能な全ての分布の中で、$\mathcal{L}(q)$を最大にするようなものを選ぶ
		
		\item $\mathcal{L}(q)$は次のように変形できる
		\begin{eqnarray}
			\mathcal{L}(q) &=& \int q_j(\bm{Z}_j) \ln \frac{\widetilde{p}(\bm{X}, \bm{Z}_j)}{q_j(\bm{Z}_j)} d\bm{Z}_j + \mathrm{Const.} \\
			&=& - \KL \left( q_j(\bm{Z}_j) || \widetilde{p}(\bm{X}, \bm{Z}_j) \right) + \mathrm{Const.}
		\end{eqnarray}
		
		\item これより、$\mathcal{L}(q)$は$q_j(\bm{Z}_j)$と$\widetilde{p}(\bm{X}, \bm{Z}_j)$の間の、負のKLダイバージェンスとなっている
		\item そして、$\mathcal{L}(q)$の$q_j(\bm{Z}_j)$に関する最大化は、\alert{KLダイバージェンスの最小化}と等価
		\newline
		\item KLダイバージェンスを最小にするためには、$q_j(\bm{Z}_j) = \widetilde{p}(\bm{X}, \bm{Z}_j)$とすればよい
		\newline
		\item 従って、$q_j(\bm{Z}_j)$の最適解は、次のように書ける
		\begin{equation}
			\color{red}\ln q_j^*(\bm{Z}_j) = \mathbb{E}_{i \neq j}[\ln p(\bm{X}, \bm{Z})] + \mathrm{Const.}\normalcolor
		\end{equation}
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{変分推論}

\begin{itemize}
	\item 下界$\mathcal{L}(q)$を最大化する$\ln q_j(\bm{Z}_j)$の解
	\begin{itemize}
		\item $q_j(\bm{Z}_j)$の最適解は次のように書けた
		\begin{equation}
			\ln q_j^*(\bm{Z}_j) = \mathbb{E}_{i \neq j}[\ln p(\bm{X}, \bm{Z})] + \mathrm{Const.}
		\end{equation}
		
		\item 上式は次のことを意味している
		\item 因子$q_j(\bm{Z}_j)$の最適解の対数$\ln q_j^*(\bm{Z}_j)$は、観測データ$\bm{X}$と潜在変数$\bm{Z}$の\alert{同時分布の対数}$\ln p(\bm{X}, \bm{Z})$\alert{を考え}、$i \neq j$である他の因子$q_i(\bm{Z}_i)$について\alert{期待値を取ったもの}である
		\newline
		
		\item 定数項は、正規化することで得られるので、結局次のようになる
		\begin{equation}
			q_j^*(\bm{Z}_j) = \frac{\exp(\mathbb{E}_{i \neq j}[\ln p(\bm{X}, \bm{Z})])}{\displaystyle \int \exp(\mathbb{E}_{i \neq j}[\ln p(\bm{X}, \bm{Z})]) d\bm{Z}_j}
		\end{equation}
		
		\item 正規化定数は必要に応じて計算すればよいので、取り敢えず無視できる
		\newline
		
		\item 最適解の式は、$q(\bm{Z})$の分解の数だけ得られるので、$\left\{ q_i(\bm{Z}_i) \right\}$に関する$M$本の連立方程式となる
		\item この方程式は、分布$q(\bm{Z})$が$M$個の因子に分解されるという仮定の下で、下界$\mathcal{L}(q)$の最大値が満たすべき条件である
		\newline
		
		\item $\ln q_j^*(\bm{Z}_j)$の右辺は、$i \neq j$である$q_i(\bm{Z}_i)$の期待値に依存するため、$q_j^*(\bm{Z}_j)$を陽に求めることができない
		\item そこで、下界$\mathcal{L}(q)$は次のように最適化される(\alert{重要})
		\newline
		\item $i \neq j$である全ての$q_i(\bm{Z}_i)$を\alert{固定}した状態で、$q_j(\bm{Z}_j)$を最適化することを、全ての$j = 1, \ldots, M$について繰り返す手続きを、\alert{座標降下法}という
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{変分推論}

\begin{itemize}
	\item 下界$\mathcal{L}(q)$の最適化 (\alert{座標降下法})
	\begin{enumerate}
		\item 全ての因子$q_j(\bm{Z}_j)$を適当に初期化する
		\newline
		\item 各因子を、以下の式を使って更新する \label{enum:variational-inference-step}
		\begin{eqnarray}
			&& \ln q_j(\bm{Z}_j) = \mathbb{E}_{i \neq j}[\ln p(\bm{X}, \bm{Z})] + \mathrm{Const.} \\
			&& \mathbb{E}_{i \neq j}[\ln p(\bm{X}, \bm{Z})] = \int \ln p(\bm{X}, \bm{Z}) \prod_{i \neq j} q_i(\bm{Z}_i) d\bm{Z}_i
		\end{eqnarray}
		即ち、因子$q_j(\bm{Z}_j)$を、他の全ての因子の現在の値$q_i(\bm{Z}_i)$を使って改良する
		\newline
		\item (\ref{enum:variational-inference-step})を、下界$\mathcal{L}(q)$が収束するまで繰り返す
	\end{enumerate}
\end{itemize}

\end{frame}

\begin{frame}{変分推論}

\begin{itemize}
	\item ここまでの話の流れ
	\begin{enumerate}
		\item $\ln p(\bm{X}) = \mathcal{L}(q) + \KL (q(\bm{Z}) || p(\bm{Z} | \bm{X}))$であるから、\alert{エビデンス下界}$\mathcal{L}(q)$を\color{red}$q$について最大化\normalcolor すれば、$\KL (q || p) = 0$とでき、従って$q(\bm{Z}) = p(\bm{Z} | \bm{X})$を得る
		\newline
		\item $q(\bm{Z}) = p(\bm{Z} | \bm{X})$が分かれば、データ$\bm{X}$から、\color{red}潜在変数やパラメータ$\bm{Z}$が得られる\normalcolor (パラメータは潜在変数$\bm{Z}$に含まれている)
		\newline
		\item しかし、事後分布$q(\bm{Z}) = p(\bm{Z} | \bm{X})$は\alert{計算不可能}なので、何らかの方法で\alert{近似}するしかない
		\newline
		\item 近似するといっても、計算可能でなければならないので、$q(\bm{Z})$の形には、通常\alert{何らかの制限}を課す
		\newline
		\item $q(\bm{Z})$を、\alert{パラメトリックな分布}$q(\bm{Z} | \bm{\omega})$と仮定することがある
		\newline
		\item または、$q(\bm{Z})$を、$\prod_i q_i(\bm{Z}_i)$のように分解できるとする(\alert{平均場近似})
		\newline
		\item \alert{平均場近似}を行うとき、各因子$q_j(\bm{Z}_j)$の最適解は、$\ln q_j^*(\bm{Z}_j) = \mathbb{E}_{i \neq j}[\ln p(\bm{X}, \bm{Z})] + \mathrm{Const.}$であった
		\newline
		\item 全ての因子$\left\{ q_j(\bm{Z}_j) \right\}$を\alert{同時に最適化することはできない}
		\newline
		\item 下界$\mathcal{L}(q)$を、各因子$q_j(\bm{Z}_j)$について\alert{順番に最適化}することはできる
	\end{enumerate} \
	
	\item これからの話の流れ
	\begin{itemize}
		\item MAP推定と最尤推定は、変分推論の\alert{特殊な場合}であることを確認する
	\end{itemize}
\end{itemize}

\end{frame}

\subsection{MAP推定および最尤推定と変分推論}

\begin{frame}{変分推論}

\begin{itemize}
	\item MAP推定および最尤推定の変分推論からの導出
	\begin{itemize}
		\item 変分推論で得たいのは、潜在変数に関する事後分布$p(\bm{Z} | \bm{X})$である
		\item この変分推論の\alert{特殊ケース}が、MAP推定や最尤推定であることを導く
		\newline
		\item 変分推論では、エビデンス下界$\mathcal{L}(q)$を$q$について最大化し、従って\color{red}$\KL(q || p)$を最小化\normalcolor する
		\item いま、分布$q(\bm{Z})$が、次の\alert{デルタ関数}であるとする
		\item 特定の値$\bm{Z} = \widehat{\bm{Z}}$についてのみ確率が非零になる、無限に鋭い分布
		\begin{equation}
			q(\bm{Z}) = \delta(\bm{Z} - \widehat{\bm{Z}})
		\end{equation}
		
		\item このとき、KLダイバージェンス$\KL(q || p)$は次のようになる
		\begin{eqnarray}
			&& \KL(q || p) \nonumber \\
			&\equiv& \KL(q(\bm{Z}) || p(\bm{Z} | \bm{X})) \nonumber \\
			&=& - \int q(\bm{Z}) \ln \frac{p(\bm{Z} | \bm{X})}{q(\bm{Z})} d\bm{Z} \nonumber \\
			&=& - \int q(\bm{Z}) \ln p(\bm{Z} | \bm{X}) d\bm{Z} + \int q(\bm{Z}) \ln q(\bm{Z}) d\bm{Z} \\
			&=& - \int \delta(\bm{Z} - \widehat{\bm{Z}}) \ln p(\bm{Z} | \bm{X}) d\bm{Z} + \nonumber \\
			&& \qquad \int \delta(\bm{Z} - \widehat{\bm{Z}}) \ln \delta(\bm{Z} - \widehat{\bm{Z}}) d\bm{Z} \\
			&=& - \ln p(\widehat{\bm{Z}} | \bm{X}) + \ln \delta(\widehat{\bm{Z}} - \widehat{\bm{Z}}) \\
			&=& - \ln p(\widehat{\bm{Z}} | \bm{X}) + \mathrm{Const.}
		\end{eqnarray}
		
		\item これを$q(\bm{Z})$について最小化することは、$\widehat{\bm{Z}}$について最小化することに相当する
		\item よって$\widehat{\bm{Z}}$の最適解$\widehat{\bm{Z}}^*$は
		\begin{eqnarray}
			\widehat{\bm{Z}}^* &=& \argmin_{\widehat{\bm{Z}}} \left( -\ln p(\widehat{\bm{Z}} | \bm{X}) \right) \\
			&=& \argmax_{\widehat{\bm{Z}}} \ln p(\widehat{\bm{Z}} | \bm{X}) \\
			&=& \argmax_{\widehat{\bm{Z}}} p(\widehat{\bm{Z}} | \bm{X}) \\
			&=& \argmax_{\widehat{\bm{Z}}} p(\bm{X} | \widehat{\bm{Z}}) p(\widehat{\bm{Z}})
		\end{eqnarray}
		となるから、\alert{MAP推定}(\alert{最大事後確率}推定)の式と一致する
		\newline
		
		\item これより、MAP推定は、KLダイバージェンス$\KL(q || p)$の最小化、従って、\color{red}エビデンス下界$\mathcal{L}(q)$の最大化と等価\normalcolor である
		\newline
		\item 更に、$p(\widehat{\bm{Z}}) = \mathrm{Const.}$、即ち$\widehat{\bm{Z}}$に関する事前の情報がないとすると
		\begin{eqnarray}
			\widehat{\bm{Z}}^* &=& \argmax_{\widehat{\bm{Z}}} p(\bm{X} | \widehat{\bm{Z}}) p(\widehat{\bm{Z}}) \nonumber \\
			&=& \argmax_{\widehat{\bm{Z}}} p(\bm{X} | \widehat{\bm{Z}})
		\end{eqnarray}
		となるから、これは\alert{最尤推定}の式に一致する
		\newline
		
		\item 下界$\mathcal{L}(q)$の式から導くことも可能である
		\begin{eqnarray}
			\mathcal{L}(q) &=& \int q(\bm{Z}) \ln \frac{p(\bm{X}, \bm{Z})}{q(\bm{Z})} d\bm{Z} \nonumber \\
			&=& \int q(\bm{Z}) \ln p(\bm{X}, \bm{Z}) d\bm{Z} - \int q(\bm{Z}) \ln q(\bm{Z}) d\bm{Z} \\
			&=& \int \delta(\bm{Z} - \widehat{\bm{Z}}) \ln p(\bm{X}, \bm{Z}) d\bm{Z} - \nonumber \\
			&& \qquad \int \delta(\bm{Z} - \widehat{\bm{Z}}) \ln \delta(\bm{Z} - \widehat{\bm{Z}}) d\bm{Z} \\
			&=& \ln p(\bm{X}, \widehat{\bm{Z}}) - \ln \delta(\widehat{\bm{Z}} - \widehat{\bm{Z}}) \\
			&=& \ln p(\bm{X}, \widehat{\bm{Z}}) + \mathrm{Const.} \\
			&=& \ln p(\bm{X} | \widehat{\bm{Z}}) p(\widehat{\bm{Z}}) + \mathrm{Const.}
		\end{eqnarray}
		
		\item これより、下界$\mathcal{L}(q)$の最大化は、$\widehat{\bm{Z}}$に関する$p(\widehat{\bm{Z}} | \bm{X}) \propto p(\bm{X} | \widehat{\bm{Z}}) p(\widehat{\bm{Z}})$の最大化、\color{red}即ちMAP推定と等価\normalcolor である
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{MAP推定の例}

\begin{itemize}
	\item スパース符号化
	\begin{itemize}
		\item ここではMAP推定の例として、スパース符号化を扱う(\alert{変分推論の例ではない})
		\newline
		\item データ$\bm{x}$に対する潜在変数$\bm{z}$に、\alert{スパース性}を持たせる
		\item そのために、スパース性を導く事前分布(\alert{ラプラス事前分布})を、潜在変数に用いる
		\newline
		
		\item データ$\bm{x}$を$D$次元、また潜在変数$\bm{z}$を$K$次元とする
		\item データ$\bm{x}$に対応する潜在変数を$\bm{z}_i$と表し、$\bm{z}_i$の第$k$成分を$z_{ik}$とする
		\begin{eqnarray}
			p(z_{ik} | \lambda) &=& \frac{\lambda}{2} \exp \left( -\lambda |z_{ik}| \right) \\
			p(\bm{z}_i | \lambda) &=& \prod_k p(z_{ik}) = \prod_k \frac{\lambda}{2} \exp \left( -\lambda |z_{ik}| \right)
		\end{eqnarray}
		
		\item データに関する事後分布$p(\bm{x}_i | \bm{z}_i, \bm{W}, \bm{b}, \beta)$を、次で定義する
		\begin{eqnarray}
			&& p(\bm{x}_i | \bm{z}_i, \bm{W}, \bm{b}, \beta) \nonumber \\
			&=& \mathcal{N}(\bm{x}_i | \bm{W} \bm{z}_i + \bm{b}, \beta^{-1} \bm{I}) \\
			&=& \frac{1}{(2\pi)^\frac{D}{2}} \frac{1}{|\beta^{-1} \bm{I}|^\frac{1}{2}} \exp \left( -\frac{1}{2} \left( \bm{x}_i - \left( \bm{W} \bm{z}_i + \bm{b} \right) \right)^T \right. \nonumber \\
			&& \qquad \left( \beta^{-1} \bm{I} \right)^{-1} \left( \bm{x}_i - \left( \bm{W} \bm{z}_i + \bm{b} \right) \right) \bigg) \\
			&=& \frac{1}{(2\pi \beta^{-1})^\frac{D}{2}} \exp \left( -\frac{\beta}{2} \left( \bm{x}_i - \left( \bm{W} \bm{z}_i + \bm{b} \right) \right)^T \right. \nonumber \\
			&& \qquad \left( \bm{x}_i - \left( \bm{W} \bm{z}_i + \bm{b} \right) \right) \bigg)
		\end{eqnarray}
		
		\item データ$\bm{x}_i$は、対応する潜在変数$\bm{z}_i$に、線形変換$\bm{W} \bm{z}_i + \bm{b}$を施し、更に分散$\beta^{-1} \bm{I}$のガウスノイズを足すことで、生成されると考える
	\end{itemize} \
	
	\item パラメータについての補足
	\begin{itemize}
		\item 今回は、$\bm{b} = 0$とおいて\alert{バイアスを無視}したものを考える
		\item $\lambda$と、精度$\beta$は\alert{ハイパーパラメータ}であり、予め決められているとする
		\item そこでこれ以降、次のように確率分布を表現する
		\begin{eqnarray}
			&& p(\bm{z}_i | \lambda) = p(\bm{z}_i) \\
			&& p(\bm{x}_i | \bm{z}_i, \bm{W}, \bm{b}, \beta) = p(\bm{x}_i | \bm{z}_i, \bm{W})
		\end{eqnarray}
		
		\item $\bm{X}$を、全てのデータ$\left\{ \bm{x}_i \right\}$を集めた行列とする(第$i$行ベクトルが$\bm{x}_i^T$)
		\item $\bm{Z}$についても同様に、全ての潜在変数$\left\{ \bm{z}_i \right\}$を集めた行列として定める(第$i$行ベクトルが$\bm{z}_i^T$)
		%\item \alert{完全にベイズ的なモデル}では、ハイパーパラメータについても事前分布を導入するが、今回はそうはしない
		%\item 完全にベイズ的であれば、パラメータは明示的に書く必要がなく、潜在変数の中に含まれる
		%\item このとき、重み$\bm{W}$も潜在変数$\bm{Z}$に含まれる($\widehat{\bm{Z}} = \left\{ \bm{W}, \bm{Z} \right\}$)
	\end{itemize} \
	
	\item スパース符号化の学習
	\begin{itemize}
		\item 事後分布$p(\bm{z}_i | \bm{x}_i)$は、\alert{表現することすら困難}であるため、最尤推定による手法(EMアルゴリズムなど)は利用できない
		\item そこで、最尤推定の代わりに\alert{MAP推定}を利用することで、最適なパラメータが得られる
		\newline
		\item 最大化するのは、以下の事後分布$p(\bm{Z} | \bm{X}, \bm{W})$である
		\begin{eqnarray}
			p(\bm{Z} | \bm{X}, \bm{W}) &=& \frac{p(\bm{X}, \bm{Z} | \bm{W})}{p(\bm{X})} \\
			&\propto& p(\bm{X}, \bm{Z} | \bm{W}) \\
			&=& p(\bm{X} | \bm{Z}, \bm{W}) p(\bm{Z}) \\
			&=& \left( \prod_i p(\bm{x}_i | \bm{z}_i, \bm{W}) \right) \left( \prod_i p(\bm{z}_i) \right) \\
			&=& \prod_i p(\bm{x}_i | \bm{z}_i, \bm{W}) p(\bm{z}_i)
		\end{eqnarray}
		
		\item 対数を取って最大化してもよいので、最大化する量は結局
		\begin{eqnarray}
			\ln p(\bm{Z} | \bm{X}, \bm{W}) &=& \ln \left( \prod_i p(\bm{x}_i | \bm{z}_i, \bm{W}) p(\bm{z}_i) \right) \\
			&=& \sum_i \left( \ln p(\bm{x}_i | \bm{z}_i, \bm{W}) + \ln p(\bm{z}_i) \right) \\
			&=& \sum_i \ln p(\bm{x}_i | \bm{z}_i, \bm{W}) + \sum_i \ln p(\bm{z}_i)
		\end{eqnarray}
		各項を求めると
		\begin{eqnarray}
			&& \sum_i \ln p(\bm{x}_i | \bm{z}_i, \bm{W}) \nonumber \\
			&=& \sum_i \ln \mathcal{N}(\bm{x}_i | \bm{W} \bm{z}_i, \beta^{-1} \bm{I}) \nonumber \\
			&=& \sum_i \ln \left( \frac{1}{(2\pi \beta^{-1})^\frac{D}{2}} \exp \left( -\frac{\beta}{2} \left( \bm{x}_i - \bm{W} \bm{z}_i \right)^T \left( \bm{x}_i - \bm{W} \bm{z}_i \right) \right) \right) \nonumber \\
			&=& \sum_i \left( -\frac{D}{2} \ln 2\pi - \frac{D}{2} \ln \beta^{-1} - \right. \nonumber \\
			&& \qquad \left. \frac{\beta}{2} \left( \bm{x}_i - \bm{W} \bm{z}_i \right)^T \left( \bm{x}_i - \bm{W} \bm{z}_i \right) \right) \nonumber \\
			&=& \sum_i \left( -\frac{\beta}{2} \left( \bm{x}_i - \bm{W} \bm{z}_i \right)^T \left( \bm{x}_i - \bm{W} \bm{z}_i \right) \right) + \mathrm{Const.} \nonumber \\
			&=& -\frac{\beta}{2} \sum_i \left( \bm{x}_i - \bm{W} \bm{z}_i \right)^T \left( \bm{x}_i - \bm{W} \bm{z}_i \right) + \mathrm{Const.} \nonumber \\
			&=& -\frac{\beta}{2} \sum_i \left( \bm{X} - \bm{Z} \bm{W}^T \right)_{i:} \left( \left( \bm{X} - \bm{Z} \bm{W}^T \right)_{i:} \right)^T + \mathrm{Const.} \\
			&=& -\frac{\beta}{2} \sum_{i, j} \left( \left( \bm{X} - \bm{Z} \bm{W}^T \right) \odot \left( \bm{X} - \bm{Z} \bm{W}^T \right) \right)_{i, j} + \mathrm{Const.} \\
			&=& -\frac{\beta}{2} \sum_{i, j} \left( \bm{X} - \bm{Z} \bm{W}^T \right)_{i, j}^2 + \mathrm{Const.} \\
			&=& -\frac{\beta}{2} || \bm{X} - \bm{Z} \bm{W}^T ||_2^2 + \mathrm{Const.}
		\end{eqnarray}
		また
		\begin{eqnarray}
			&& \sum_i \ln p(\bm{z}_i) \nonumber \\
			&=& \sum_i \ln \prod_j \left( \frac{\lambda}{2} \exp \left( -\lambda |z_{ij}| \right) \right) \nonumber \\
			&=& \sum_i \sum_j \left( \ln \frac{\lambda}{2} - \lambda |z_{ij}| \right) \nonumber \\
			&=& -\lambda \sum_{i, j} |z_{ij}| + \mathrm{Const.} \\
			&=& -\lambda \sum_{i, j} |\bm{Z}_{i, j}| + \mathrm{Const.} \\
			&=& -\lambda || \bm{Z} ||_1 + \mathrm{Const.}
		\end{eqnarray}
		のようになる
		\newline
		
		\item $\odot$はアダマール積、$\bm{A}_{i:}$は行列$\bm{A}$の第$i$行目のベクトルを表す
		
		\item また、行列のノルム$|| \bm{A} ||_p$は次で定義される
		\begin{equation}
			|| \bm{A} ||_p = \left( \sum_{i, j} |A_{i, j}|^p \right)^\frac{1}{p}
		\end{equation}
		$p = 1, p = 2$のときは次のようになる
		\begin{equation}
			|| \bm{A} ||_1 = \sum_{i, j} |A_{i, j}|, \quad || \bm{A} ||_2 = \sqrt{\sum_{i, j} A_{i, j}^2}
		\end{equation}
		
		\item これより、$|| \bm{A} ||_1$は、\color{red}行列$\bm{A}$の各要素の絶対値の和\normalcolor を表す
		\item また$|| \bm{A} ||_2$は、行列$\bm{A}$の各要素の二乗和の平方根を表し、\alert{フロベニウスノルム}$|| \bm{A} ||_F$ともよばれる
		\item 従って、$|| \bm{A} ||_2^2$は、\color{red}行列$\bm{A}$の各要素の二乗和\normalcolor である
		\newline
		\item 上記から
		\begin{eqnarray}
			&& \ln p(\bm{Z} | \bm{X}, \bm{W}) \nonumber \\
			&=& \sum_i \ln p(\bm{x}_i | \bm{z}_i, \bm{W}) + \sum_i \ln p(\bm{z}_i) \nonumber \\
			&=& -\frac{\beta}{2} || \bm{X} - \bm{Z} \bm{W}^T ||_2^2 - \lambda || \bm{Z} ||_1 + \mathrm{Const.}
		\end{eqnarray}
		となるので、$\ln p(\bm{Z} | \bm{X}, \bm{W})$の最大化は、以下の\color{red}関数$J(\bm{Z}, \bm{W})$の最小化\normalcolor である
		\begin{equation}
			J(\bm{Z}, \bm{W}) = || \bm{X} - \bm{Z} \bm{W}^T ||_2^2 + || \bm{Z} ||_1
		\end{equation}
	\end{itemize} \
		
	\item スパース符号化の学習方法のまとめ
	\begin{itemize}
		\item 関数$J(\bm{Z}, \bm{W})$を、$\bm{Z}$と$\bm{W}$について\alert{交互に最小化}すればよい
		\newline
		\item $p(\bm{Z} | \bm{X}, \bm{W})$の最大化は、$p(\bm{X} | \bm{Z}, \bm{W}) p(\bm{Z})$の最大化、即ち$J(\bm{Z}, \bm{W})$の最小化と等価であった
		\item 従って、$J(\bm{Z}, \bm{W})$の各パラメータ$\bm{Z}, \bm{W}$についての最小化は、事後確率を大きくする方向に働く
		\newline
		\item 関数$J(\bm{Z}, \bm{W})$をもう一度みてみよう
		\begin{equation}
			J(\bm{Z}, \bm{W}) = || \bm{X} - \bm{Z} \bm{W}^T ||_2^2 + || \bm{Z} ||_1
		\end{equation}
		
		\item 第1項は、明らかに\alert{再構成誤差}を表している
		\item 第2項は、データ$\bm{X}$の表現$\bm{Z}$がスパースになるように付加した、\alert{正則化項}である
		\newline
		\item $\bm{Z} \bm{W}^T$は、データ$\bm{X}$の内部表現$\bm{Z}$に、重み$\bm{W}$を掛けることで、データ$\bm{X}$を復元したもの
	\end{itemize}
\end{itemize}

\end{frame}

\subsection{変分近似による弊害}

\begin{frame}{変分推論}

\begin{itemize}
	\item これまでの話の流れ
	\begin{enumerate}
		\item 変分推論から、MAP推定と最尤推定が導出できることを確認した
		\newline
		\item MAP推定は、$q(\bm{Z})$を無限に鋭い確率分布(デルタ関数)として、$\bm{Z}$が特定の値しか取らないと仮定した場合であった
		\item 最尤推定は、MAP推定において、$\bm{Z}$の事前確率分布を設けない場合であった
		\newline
		\item MAP推定の例として、スパース符号化を扱った
	\end{enumerate} \
	
	\item これからの話の流れ
	\begin{itemize}
		\item 話題を変えて、分解$q(\bm{Z}) = \prod_i q_i(\bm{Z}_i)$によって$p(\bm{Z} | \bm{X})$を近似するときの、弊害を調べる
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{変分推論}

\begin{itemize}
	\item $q(\bm{Z})$の分解による近似の性質
	\begin{itemize}
		\item 変分推論では、真の事後分布$q(\bm{Z}) = p(\bm{Z} | \bm{X})$を、分解により近似する
		\item 分解で近似することによって、\alert{どのような不正確さが生じるのか?}
	\end{itemize} \
	
	\item ガウス分布の分解による近似
	\begin{itemize}
		\item ガウス分布を、\alert{分解されたガウス分布}で近似することを考えてみよう
		\item 分解による近似で、どのような問題が起こるのか考えてみよう
		\newline
		\item 2つの変数$\bm{z} = (z_1, z_2)$間には、\alert{相関がある}とする
		\item $\bm{z}$はガウス分布$p(\bm{z}) = \mathcal{N}(\bm{z} | \bm{\mu}, \bm{\Lambda}^{-1})$に従っているとする
		\begin{equation}
			\bm{\mu} = \left[ \begin{array}{l} \mu_1 \\ \mu_2 \end{array} \right], \qquad \bm{\Lambda} = \left[ \begin{array}{ll} \Lambda_{11} & \Lambda_{12} \\ \Lambda_{21} & \Lambda_{22} \end{array} \right]
		\end{equation}
		
		\item 精度行列$\bm{\Lambda}$は対称行列であるから、$\Lambda_{12} = \Lambda_{21}$
		\newline
		\item この分布$p(\bm{z})$を、分解されたガウス分布$q(\bm{z}) = q_1(z_1) q_2(z_2)$で近似する
		\item 各因子$q_1(z_1), q_2(z_2)$の関数形については\alert{何の仮定も置いていない}ことに注意
	\end{itemize} \
	
	\item 最適な因子$q_1(z_1), q_2(z_2)$の計算
	\begin{itemize}
		\item 最適な因子$q_1^*(z_1)$を、先程の結果を使って求める
		\begin{equation}
			\ln q_j(\bm{Z}_j) = \mathbb{E}_{i \neq j}[\ln p(\bm{X}, \bm{Z})] + \mathrm{Const.}
		\end{equation}
		
		\item 従って、$q_1^*(z_1)$を計算する式は次のようになる
		\begin{eqnarray}
			&& \ln q_1^*(z_1) = \mathbb{E}_{z_2}[\ln p(\bm{z})] + \mathrm{Const.} \\
			&& \mathbb{E}_{z_2}[\ln p(\bm{z})] = \int \ln p(\bm{z}) q_2(z_2) dz_2
		\end{eqnarray}
		
		\item 上式の右辺では、$z_1$に依存する項だけを考えればよい
		\item $z_1$の関数を求めようとしているため
		\item $z_1$に依存しない項は、全て定数項(正規化定数)に含まれてしまうため
		\newline
		\item 従って$q_1^*(z_1)$は
		\begin{eqnarray}
			\ln q_1^*(z_1) &=& \mathbb{E}_{z_2}[\ln p(\bm{z})] + \mathrm{Const.} \\
			&=& \mathbb{E}_{z_2} \left[ \ln \mathcal{N}(\bm{z} | \bm{\mu}, \bm{\Lambda}^{-1}) \right] + \mathrm{Const.}
		\end{eqnarray}
		但し
		\begin{eqnarray}
			&& \ln \mathcal{N}(\bm{z} | \bm{\mu}, \bm{\Lambda}^{-1}) \nonumber \\
			&=& \ln \left( \frac{1}{(2\pi)^\frac{2}{2}} \frac{1}{|\bm{\Lambda}^{-1}|^\frac{1}{2}} \exp \left( -\frac{1}{2} (\bm{z} - \bm{\mu})^T \left( \bm{\Lambda}^{-1} \right)^{-1} (\bm{z} - \bm{\mu}) \right) \right) \nonumber \\
			&=& \ln \left( \frac{1}{2\pi} \frac{1}{|\bm{\Lambda}|^{-\frac{1}{2}}} \exp \left( -\frac{1}{2} (\bm{z} - \bm{\mu})^T \bm{\Lambda} (\bm{z} - \bm{\mu}) \right) \right) \nonumber \\
			&=& -\frac{1}{2} (\bm{z} - \bm{\mu})^T \bm{\Lambda} (\bm{z} - \bm{\mu}) + \mathrm{Const.}
		\end{eqnarray}
		$z_1$に依存する項だけを取り出せば
		\begin{eqnarray}
			&& -\frac{1}{2} (\bm{z} - \bm{\mu})^T \bm{\Lambda} (\bm{z} - \bm{\mu}) \nonumber \\
			&=& -\frac{1}{2} \left[ z_1 - \mu_1, z_2 - \mu_2 \right] \left[ \begin{array}{ll} \Lambda_{11} & \Lambda_{12} \\ \Lambda_{21} & \Lambda_{22} \end{array} \right] \left[ \begin{array}{l} z_1 - \mu_1 \\ z_2 - \mu_2 \end{array} \right] \nonumber \\
			&=& -\frac{1}{2} \bigg[ \Big\{ (\Lambda_{11} (z_1 - \mu_1) + \Lambda_{21} (z_2 - \mu_2) \Big\} (z_1 - \mu_1) + \nonumber \\
			&& \qquad \Big\{ \Lambda_{12} (z_1 - \mu_1) + \Lambda_{22} (z_2 - \mu_2) \Big\} (z_2 - \mu_2) \bigg] \nonumber \\
			&=& -\frac{1}{2} \left( \Lambda_{11} (z_1 - \mu_1)^2 + 2 \Lambda_{12} (z_1 - \mu_1)(z_2 - \mu_2) + \right. \nonumber \\
			&& \qquad \left. \Lambda_{22} (z_2 - \mu_2)^2 \right) \quad (\because \Lambda_{21} = \Lambda_{12}) \nonumber \\
			&=& -\frac{1}{2} \Lambda_{11} (z_1 - \mu_1)^2 - \Lambda_{12} (z_1 - \mu_1)(z_2 - \mu_2) + \mathrm{Const.}
		\end{eqnarray}
		これを代入して
		\begin{eqnarray}
			&& \ln \mathcal{N}(\bm{z} | \bm{\mu}, \bm{\Lambda}^{-1}) \nonumber \\
			&=& -\frac{1}{2} \Lambda_{11} (z_1 - \mu_1)^2 - \Lambda_{12} (z_1 - \mu_1)(z_2 - \mu_2) + \mathrm{Const.}
		\end{eqnarray}
		従って
		\begin{eqnarray}
			&& \ln q_1^*(z_1) \nonumber \\
			&=& \mathbb{E}_{z_2} \left[ \ln \mathcal{N}(\bm{z} | \bm{\mu}, \bm{\Lambda}^{-1}) \right] + \mathrm{Const.} \nonumber \\
			&=& \mathbb{E}_{z_2} \left[ -\frac{1}{2} \Lambda_{11} (z_1 - \mu_1)^2 - \Lambda_{12} (z_1 - \mu_1)(z_2 - \mu_2) \right] + \mathrm{Const.} \nonumber \\
			&=& -\frac{1}{2} \Lambda_{11} z_1^2 + \Lambda_{11} \mu_1 z_1 - \Lambda_{12} z_1 \left( \mathbb{E}[z_2] - \mu_2 \right) + \mathrm{Const.}
		\end{eqnarray}
		
		\item これより、$q_1^*(z_1)$は次のように書ける
		\begin{eqnarray}
			&& q_1^*(z_1) \nonumber \\
			&\propto& \exp \left( -\frac{1}{2} \Lambda_{11} z_1^2 + \Lambda_{11} \mu_1 z_1 - \Lambda_{12} z_1 \left( \mathbb{E}[z_2] - \mu_2 \right) \right) \nonumber \\
			&=& \exp \left( -\frac{1}{2} \Lambda_{11} \left( z_1 - (\mu_1 - \Lambda_{11}^{-1} \Lambda_{12} (\mathbb{E}[z_2] - \mu_2)) \right)^2 + \right. \nonumber \\
			&& \qquad \left. \frac{1}{2} \Lambda_{11} \left( \mu_1 - \Lambda_{11}^{-1} \Lambda_{12} (\mathbb{E}[z_2] - \mu_2) \right)^2 \right) \nonumber \\
			&\propto& \exp \left( -\frac{1}{2 \Lambda_{11}^{-1}} \left( z_1 - (\mu_1 - \Lambda_{11}^{-1} \Lambda_{12} (\mathbb{E}[z_2] - \mu_2)) \right)^2 \right) \nonumber \\
			&=& \mathcal{N}(z_1 | m_1, \Lambda_{11}^{-1})
		\end{eqnarray}
		但し$m_1$は次のようにおいた
		\begin{equation}
			m_1 = \mu_1 - \Lambda_{11}^{-1} \Lambda_{12} (\mathbb{E}[z_2] - \mu_2)
		\end{equation}
		
		\item 対称性から、$q_2^*(z_2)$も次のように求められる
		\begin{eqnarray}
			\ln q_2^*(z_2) &=& \mathbb{E}_{z_2}[\ln p(\bm{z})] + \mathrm{Const.} \\
			&=& \mathbb{E}_{z_2}[\ln \mathcal{N}(\bm{z} | \bm{\mu}, \bm{\Lambda}^{-1})] + \mathrm{Const.}
		\end{eqnarray}
		\begin{equation}
			q_2^*(z_2) = \mathcal{N}(z_2 | m_2, \Lambda_{22}^{-1})
		\end{equation}
		但し$m_2$は次のようにおいた
		\begin{equation}
			m_2 = \mu_2 - \Lambda_{22}^{-1} \Lambda_{21} (\mathbb{E}[z_1] - \mu_1)
		\end{equation}
		
		\item これより$q_1^*(z_1), q_2^*(z_2)$は
		\begin{eqnarray}
			q_1^*(z_1) &=& \mathcal{N}(z_1 | m_1, \Lambda_{11}^{-1}) \\
			m_1 &=& \mu_1 - \Lambda_{11}^{-1} \Lambda_{12} (\mathbb{E}[z_2] - \mu_2) \\
			q_2^*(z_2) &=& \mathcal{N}(z_2 | m_2, \Lambda_{22}^{-1}) \\
			m_2 &=& \mu_2 - \Lambda_{22}^{-1} \Lambda_{21} (\mathbb{E}[z_1] - \mu_1)
		\end{eqnarray}
		
		\item $\mathbb{E}[z_2]$は、$z_2$の$q_2(z_2) = \mathcal{N}(z_2 | m_2, \Lambda_{22}^{-1})$による平均であるから、$\mathbb{E}[z_2] = m_2$である(同様に、$\mathbb{E}[z_1] = z_1$)
		\newline
		
		\item これより$m_1, m_2$を連立させれば
		\begin{eqnarray}
			m_1 &=& \mu_1 - \Lambda_{11}^{-1} \Lambda_{12} (m_2 - \mu_2) \\
			m_2 &=& \mu_2 - \Lambda_{22}^{-1} \Lambda_{21} (m_1 - \mu_1)
		\end{eqnarray}
		であるから、$m_1$について解けば
		\begin{eqnarray}
			m_1 &=& \mu_1 - \Lambda_{11}^{-1} \Lambda_{12} (m_2 - \mu_2) \\
			&=& \mu_1 - \Lambda_{11}^{-1} \Lambda_{12} \left( \mu_2 - \Lambda_{22}^{-1} \Lambda_{21} (m_1 - \mu_1) - \mu_2 \right) \\
			&=& \mu_1 + \Lambda_{11}^{-1} \Lambda_{12} \Lambda_{22}^{-1} \Lambda_{21} (m_1 - \mu_1) \\
			&=& \mu_1 + \Lambda_{11}^{-1} \Lambda_{22}^{-1} \Lambda_{12}^2 (m_1 - \mu_1) \\
			&=& \left( 1 - \Lambda_{11}^{-1} \Lambda_{22}^{-1} \Lambda_{12}^2 \right) \mu_1 + \Lambda_{11}^{-1} \Lambda_{22}^{-1} \Lambda_{12}^2 m_1
		\end{eqnarray}
		従って
		\begin{equation}
			\left( 1 - \Lambda_{11}^{-1} \Lambda_{22}^{-1} \Lambda_{12}^2 \right) \mu_1 = \left( 1 - \Lambda_{11}^{-1} \Lambda_{22}^{-1} \Lambda_{12}^2 \right) m_1
		\end{equation}
		精度行列$\bm{\Lambda}$が正則であれば
		\begin{eqnarray}
			&& \Lambda_{11} \Lambda_{12} - \Lambda_{12} \Lambda_{21} \neq 0 \\
			&\Rightarrow& \Lambda_{11} \Lambda_{12} - \Lambda_{12}^2 \neq 0 \\
			&\Rightarrow& \left( \Lambda_{11} \Lambda_{12} \right) \left( 1 - \Lambda_{11}^{-1} \Lambda_{12}^{-1} \Lambda_{12}^2 \right) \neq 0 \\
			&\Rightarrow& \left( 1 - \Lambda_{11}^{-1} \Lambda_{12}^{-1} \Lambda_{12}^2 \right) \neq 0
		\end{eqnarray}
		が成立するから、分布$p(\bm{z})$が非特異(精度行列が正則)ならば
		\begin{equation}
			m_1 = \mu_1
		\end{equation}
		が唯一の解であるほか、対称性から、$m_2$についても以下を得る
		\begin{equation}
			m_2 = \mu_2
		\end{equation}
		
		\item ゆえに、$q_1^*(z_1), q_2^*(z_2)$は
		\begin{eqnarray}
			q_1^*(z_1) &=& \color{red}\mathcal{N}(z_1 | \mu_1, \Lambda_{11}^{-1})\normalcolor \\
			q_2^*(z_2) &=& \color{red}\mathcal{N}(z_2 | \mu_2, \Lambda_{22}^{-1})\normalcolor
		\end{eqnarray}
	\end{itemize} \
	
	\item 注意点1
	\begin{itemize}
		\item $q_1^*(z_1)$は、$q_2^*(z_2)$を使って計算される$p(\bm{z})$の期待値$\mathbb{E}[z_2]$に依存 (逆も成り立つ)
		\item $q_1^*(z_1)$と、$q_2^*(z_2)$は\alert{相互に依存している}ため、2つを同時に求めることはできない
		\item その代わりに、次のように最適化すればよい
		\newline
		\item $q_1(z_1), q_2(z_2)$を適当に初期化したあと、$q_1^*(z_1), q_2^*(z_2)$の式を使って、交互に$q_1(z_1)$と$q_2(z_2)$を更新していく(収束するまでこれを繰り返す)
	\end{itemize} \
	
	\item 注意点2
	\begin{itemize}
		\item $q_1(z_1), q_2(z_2)$の具体的な関数形については、\alert{何の仮定も置かなかった}
		\item $q_i^*(z_i)$がガウス分布だという仮定は置いていないが、$\KL (q || p)$を最適化する\alert{変分推論}によって、\alert{結果的にガウス分布が得られた}
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{変分推論}

\begin{itemize}
	\item $\KL(q || p)$の最適化と$\KL(p || q)$の最適化の比較
	\begin{itemize}
		\item 上記の結果は、$\KL(q || p)$の最適化(エビデンス下界$\mathcal{L}(q)$の最適化)によって得た
		\item $\KL(q || p)$ではなく、\color{red}$\KL(p || q)$を最適化したらどうなるか?\normalcolor
		\item 変分推論ではない、もう一つの近似推論の方法である、\alert{EP法}で使われる考え方
		\newline
		\item $q(\bm{Z})$を$p(\bm{Z} | \bm{X})$に近づけたいのであれば、$\KL(q || p)$と$\KL(p || q)$のどちらを最小化してもよいはず
		\item なぜなら、KLダイバージェンスは、確率分布間の(擬似的な)距離を表すため
	\end{itemize} \
	
	\item $\KL(p || q)$の最適化
	\begin{itemize}
		\item $q(\bm{Z})$が平均場近似によって分解できるとき、$\KL(p || q)$を最適化したい
		\newline
		\item KLダイバージェンス$\KL(p || q)$は、次のように書ける
		\begin{eqnarray}
			\KL(p || q) &\equiv& \KL(p(\bm{Z} | \bm{X}) || q(\bm{Z})) \\
			&=& -\int p(\bm{Z} | \bm{X}) \ln \frac{q(\bm{Z})}{p(\bm{Z} | \bm{X})} d\bm{Z} \\
			&=& -\int p(\bm{Z} | \bm{X}) \left( \ln q(\bm{Z}) - \ln p(\bm{Z} | \bm{X}) \right) d\bm{Z} \\
			&=& -\int p(\bm{Z} | \bm{X}) \ln q(\bm{Z}) d\bm{Z} - \nonumber \\
			&& \qquad -\underbrace{\int p(\bm{Z} | \bm{X}) \ln p(\bm{Z} | \bm{X}) d\bm{Z}}_{\text{$q$には依存しない定数項}} \\
			&=& -\int p(\bm{Z} | \bm{X}) \ln \prod_i q_i(\bm{Z}_i) d\bm{Z} + \mathrm{Const.} \\
			&=& -\int p(\bm{Z} | \bm{X}) \sum_i \ln q_i(\bm{Z}_i) d\bm{Z} + \mathrm{Const.} \\
			&=& -\sum_i \int p(\bm{Z} | \bm{X}) \ln q_i(\bm{Z}_i) d\bm{Z} + \mathrm{Const.}
		\end{eqnarray}
		定数項は、$p(\bm{Z} | \bm{X})$のエントロピーであり、$q$には依存しない
		\newline
		
		\item 各因子$q_j(\bm{Z}_j)$について$\KL(p || q)$を最適化したい
		\item このとき、$i \neq j$となる、全ての$q_i(\bm{Z}_i)$は\alert{固定する}
		\newline
		\item $q_j(\bm{Z}_j)$に依存する項を取り出せば、次のようになる
		\begin{eqnarray}
			&& \sum_i \int p(\bm{Z} | \bm{X}) \ln q_i(\bm{Z}_i) d\bm{Z} \nonumber \\
			&=& \int p(\bm{Z} | \bm{X}) \ln q_j(\bm{Z}_j) d\bm{Z} \\
			&=& \int p(\bm{Z} | \bm{X}) \ln q_j(\bm{Z}_j) d\bm{Z}_1 d\bm{Z}_2 \cdots d\bm{Z}_M \\
			&=& \int p(\bm{Z} | \bm{X}) \ln q_j(\bm{Z}_j) d\bm{Z}_j \left( \prod_{i \neq j} d\bm{Z}_i \right) \\
			&=& \int \left( \int p(\bm{Z} | \bm{X}) \prod_{i \neq j} d\bm{Z}_i \right) \ln q_j(\bm{Z}_j) d\bm{Z}_j
		\end{eqnarray}
		
		\item $q_j(\bm{Z}_j)$は確率分布であるから、以下の条件を満たさなければならない
		\begin{eqnarray}
			&& \int q_j(\bm{Z}_j) d\bm{Z}_j = 1 \quad (\color{red}\text{規格化条件}\normalcolor ) \\
			&& q_j(\bm{Z}_j) \ge 0
		\end{eqnarray}
		
		\item 従って$\KL(p || q)$を$q_j(\bm{Z}_j)$について最適化するとき、\alert{ラグランジュの未定乗数法}を使って、規格化条件を組み込む必要がある
		\newline
		\item $q_j(\bm{Z}_j) \ge 0$という条件は、$\ln q_i(\bm{Z}_i)$という項が既にあるから、何もしなくても\alert{常に満たされる} (ラグランジュ関数に、制約条件を改めて取り入れる必要がない)
		\newline
		
		\item 結局、ラグランジュ汎関数$\mathcal{L}[q_j]$は、次のようになる
		\begin{eqnarray}
			\mathcal{L}[q_j] &=& - \int \left( \int p(\bm{Z} | \bm{X}) \prod_{i \neq j} d\bm{Z}_i \right) \ln q_j(\bm{Z}_j) d\bm{Z}_j + \nonumber \\
			&& \lambda \left( \int q_j(\bm{Z}_j) d\bm{Z}_j - 1 \right)
		\end{eqnarray}
		
		\item 上記は$q_j(\bm{Z}_j)$についての\alert{汎関数}となっていることに注意
		\newline
		
		\item 次の公式を使って、$\mathcal{L}[q_j]$を変分最適化する
		\begin{equation}
			\frac{\delta}{\delta y(x)} \int G(y(x), x) dx = \frac{\partial}{\partial y} G(y(x), x)
		\end{equation}
		
		\item 従って
		\begin{eqnarray}
			&& \frac{\delta}{\delta q_j(\bm{Z}_j)} \mathcal{L}[q_j] \nonumber \\
			&=& - \frac{\delta}{\delta q_j(\bm{Z}_j)} \int \left( \int p(\bm{Z} | \bm{X}) \prod_{i \neq j} d\bm{Z}_i \right) \ln q_j(\bm{Z}_j) d\bm{Z}_j + \nonumber \\
			&& \qquad \frac{\delta}{\delta q_j(\bm{Z}_j)} \lambda \left( \int q_j(\bm{Z}_j) d\bm{Z}_j - 1 \right) \\
			&=& - \frac{\partial}{\partial q_j} \left( \int p(\bm{Z} | \bm{X}) \prod_{i \neq j} d\bm{Z}_i \right) \ln q_j(\bm{Z}_j) + \nonumber \\
			&& \qquad \lambda \frac{\partial}{\partial q_j} q_j(\bm{Z}_j) \\
			&=& - \left( \int p(\bm{Z} | \bm{X}) \prod_{i \neq j} d\bm{Z}_i \right) \frac{1}{q_j(\bm{Z}_j)} + \lambda = 0
		\end{eqnarray}
		
		\item これより、未定乗数$\lambda$は
		\begin{eqnarray}
			&& \lambda q_j(\bm{Z}_j) = \left( \int p(\bm{Z} | \bm{X}) \prod_{i \neq j} d\bm{Z}_i \right) \frac{1}{q_j(\bm{Z}_j)} \\
			&\Rightarrow& \int \lambda q_j(\bm{Z}_j) d\bm{Z}_j = \int \underbrace{\left( \int p(\bm{Z} | \bm{X}) \prod_{i \neq j} d\bm{Z}_i \right)}_{=p(\bm{Z}_j | \bm{X})} d\bm{Z}_j \\
			&\Rightarrow& \lambda \underbrace{\int q_j(\bm{Z}_j) d\bm{Z}_j}_{=1} = \underbrace{\int p(\bm{Z}_j | \bm{X}) d\bm{Z}_j}_{=1} \\
			&\Rightarrow& \lambda = 1
		\end{eqnarray}
		
		\item 結局、最適解$q_j^*(\bm{Z}_j)$は次のようになる
		\begin{eqnarray}
			&& - \left( \int p(\bm{Z} | \bm{X}) \prod_{i \neq j} d\bm{Z}_i \right) \frac{1}{q_j(\bm{Z}_j)} + \lambda = 0 \\
			&\Rightarrow& q_j^*(\bm{Z}_j) = \underbrace{\int p(\bm{Z} | \bm{X}) \prod_{i \neq j} d\bm{Z}_i}_{=p(\bm{Z}_j | \bm{X})} \\
			&\Rightarrow& q_j^*(\bm{Z}_j) = p(\bm{Z}_j | \bm{X})
		\end{eqnarray}
		
		\item $q_j^(\bm{Z}_j)$の最適解は、$p(\bm{Z} | \bm{X})$を、$i \neq j$である全ての$\bm{Z}_i$について周辺化した分布
		\item これは閉じた解であり、繰り返しを必要としない
	\end{itemize} \
	
	\item 最適な因子$q_1(z_1), q_2(z_2)$の計算
	\begin{itemize}
		\item 今回は$p(\bm{z} | \bm{x}) = p(\bm{z})$の場合を考えており、かつ$p(\bm{z}) = \mathcal{N}(\bm{z} | \bm{\mu}, \bm{\Lambda}^{-1})$であった
		\newline
		
		\item 従って、$q_1^*(z_1)$は、$p(\bm{z})$を$z_2$について周辺化すればよいから
		\begin{eqnarray}
			&& q_1^*(z_1) \nonumber \\
			&=& \int p(\bm{z}) dz_2 \nonumber \\
			&=& \int \mathcal{N}(\bm{z} | \bm{\mu}, \bm{\Lambda}^{-1}) dz_2 \\
			&=& \frac{1}{2\pi} \frac{1}{|\bm{\Lambda}|^{-\frac{1}{2}}} \int \exp \left( -\frac{1}{2} (\bm{z} - \bm{\mu})^T \bm{\Lambda} (\bm{z} - \bm{\mu}) \right) dz_2
		\end{eqnarray}
		
		\item ここで、指数の内側を、積分変数$z_2$に依存する項と、そうでない項に分ける
		\begin{eqnarray}
			&& -\frac{1}{2} (\bm{z} - \bm{\mu})^T \bm{\Lambda} (\bm{z} - \bm{\mu}) \nonumber \\
			&=& -\frac{1}{2} \left( \Lambda_{11} (z_1 - \mu_1)^2 + \right. \nonumber \\
			&& \qquad \left. 2 \Lambda_{12} (z_1 - \mu_1)(z_2 - \mu_2) + \Lambda_{22} (z_2 - \mu_2)^2 \right) \\
			&=& -\frac{1}{2} \Lambda_{11} (z_1 - \mu_1)^2 + \Lambda_{12} (z_1 - \mu_1) \mu_2 + \nonumber \\
			&& \qquad -\frac{1}{2} \left( 2 \Lambda_{12} (z_1 - \mu_1) z_2 + \Lambda_{22} (z_2 - \mu_2)^2 \right)
		\end{eqnarray}
		そして
		\begin{eqnarray}
			&& -\frac{1}{2} \left( 2 \Lambda_{12} (z_1 - \mu_1) z_2 + \Lambda_{22} (z_2 - \mu_2)^2 \right) \nonumber \\
			&=& -\frac{1}{2} \left( \Lambda_{22} z_2^2 - 2 \Lambda_{22} \mu_2 z_2 + 2 \Lambda_{12} (z_1 - \mu_1) z_2 + \Lambda_{22} \mu_2^2 \right) \\
			&=& -\frac{1}{2} \left( \Lambda_{22} z_2^2 - 2 \left( \Lambda_{22} \mu_2 - \Lambda_{12} (z_1 - \mu_1) \right) z_2 + \Lambda_{22} \mu_2^2 \right) \\
			&=& -\frac{1}{2} \left( \Lambda_{22} \left( z_2 - \Lambda_{22}^{-1} \left( \Lambda_{22} \mu_2 - \Lambda_{12} (z_1 - \mu_1) \right) \right)^2 - \right. \nonumber \\
			&& \qquad \left. \Lambda_{22} \left( \Lambda_{22}^{-1} \left( \Lambda_{22} \mu_2 - \Lambda_{12} (z_1 - \mu_1) \right) \right)^2 + \Lambda_{22} \mu_2^2 \right) \\
			&=& -\frac{1}{2} \left( \Lambda_{22} \left( z_2 - \Lambda_{22}^{-1} m \right)^2 - \Lambda_{22}^{-1} m^2 + \Lambda_{22} \mu_2^2 \right)
		\end{eqnarray}
		ゆえ($m = \Lambda_{22} \mu_2 - \Lambda_{12} (z_1 - \mu_1)$とおいた)
		\begin{eqnarray}
			&& -\frac{1}{2} (\bm{z} - \bm{\mu})^T \bm{\Lambda} (\bm{z} - \bm{\mu}) \nonumber \\
			&=& -\frac{1}{2} \Lambda_{11} (z_1 - \mu_1)^2 + \Lambda_{12} (z_1 - \mu_1) \mu_2 - \frac{1}{2} \Lambda_{22} \mu_2^2 - \nonumber \\
			&& \qquad \frac{1}{2} \Lambda_{22} \left( z_2 - \Lambda_{22}^{-1} m \right)^2 + \frac{1}{2} \Lambda_{22}^{-1} m^2
		\end{eqnarray}
		
		\item これより、積分変数$z_2$の依存項だけを取り出せたので
		\begin{eqnarray}
			&& \int \exp \left( -\frac{1}{2} (\bm{z} - \bm{\mu})^T \bm{\Lambda} (\bm{z} - \bm{\mu}) \right) dz_2 \nonumber \\
			&=& \exp \left( -\frac{1}{2} \Lambda_{11} (z_1 - \mu_1)^2 + \Lambda_{12} (z_1 - \mu_1) \mu_2 - \right. \nonumber \\
			&& \qquad \left. \frac{1}{2} \Lambda_{22} \mu_2^2 + \frac{1}{2} \Lambda_{22}^{-1} m^2 \right) \nonumber \\
			&& \qquad \int \exp \left( - \frac{1}{2} \Lambda_{22} \left( z_2 - \Lambda_{22}^{-1} m \right)^2 \right) dz_2
		\end{eqnarray}
		であって、右側の積分は、中身が(正規化されていない)ガウス分布であるから
		\begin{eqnarray}
			&& \int \exp \left( - \frac{1}{2} \Lambda_{22} \left( z_2 - \Lambda_{22}^{-1} m \right)^2 \right) dz_2 \nonumber \\
			&=& (2\pi \Lambda_{22}^{-1})^\frac{1}{2} \cdot \int \frac{1}{(2\pi \Lambda_{22}^{-1})^\frac{1}{2}} \exp \left( - \frac{1}{2} \Lambda_{22} \left( z_2 - \Lambda_{22}^{-1} m \right)^2 \right) dz_2 \nonumber \\
			&=& \left( 2\pi \Lambda_{22}^{-1} \right)^\frac{1}{2}
		\end{eqnarray}
		となって、$z_2$を積分により消去できる
		\newline
		
		\item また指数の残りの部分から、$z_1$に依存する項だけを取り出して
		\begin{eqnarray}
			&& -\frac{1}{2} \Lambda_{11} (z_1 - \mu_1)^2 + \Lambda_{12} (z_1 - \mu_1) \mu_2 - \frac{1}{2} \Lambda_{22} \mu_2^2 + \frac{1}{2} \Lambda_{22}^{-1} m^2 \nonumber \\
			&=& -\frac{1}{2} \Lambda_{11} (z_1 - \mu_1)^2 + \Lambda_{12} (z_1 - \mu_1) \mu_2 - \frac{1}{2} \Lambda_{22} \mu_2^2 + \nonumber \\
			&& \qquad \frac{1}{2} \Lambda_{22}^{-1} \left( \Lambda_{22} \mu_2 - \Lambda_{12} (z_1 - \mu_1) \right)^2 \nonumber \\
			&=& -\frac{1}{2} \Lambda_{11} (z_1 - \mu_1)^2 + \Lambda_{12} (z_1 - \mu_1) \mu_2 - \frac{1}{2} \Lambda_{22} \mu_2^2 + \nonumber \\
			&& \qquad \frac{1}{2} \Lambda_{22} \mu_2^2 - \mu_2 \Lambda_{12} (z_1 - \mu_1) + \nonumber \\
			&& \qquad \frac{1}{2} \Lambda_{22}^{-1} \Lambda_{12}^2 (z_1 - \mu_1)^2 \\
			&=& -\frac{1}{2} \left( \Lambda_{11} - \Lambda_{22}^{-1} \Lambda_{12}^2 \right) (z_1 - \mu_1)^2 \\
			&=& -\frac{1}{2} \left( \Lambda_{11} - \Lambda_{22}^{-1} \Lambda_{12}^2 \right) z_1^2 + \nonumber \\
			&& \qquad \frac{1}{2} \left( \Lambda_{11} - \Lambda_{22}^{-1} \Lambda_{12}^2 \right) \mu_1 z_1 + \mathrm{Const.}
		\end{eqnarray}
		
		\item これより結局、$z_2$による積分は次のようになる
		\begin{eqnarray}
			&& \int \exp \left( -\frac{1}{2} (\bm{z} - \bm{\mu})^T \bm{\Lambda} (\bm{z} - \bm{\mu}) \right) dz_2 \nonumber \\
			&=& \left( 2\pi \Lambda_{22}^{-1} \right)^\frac{1}{2} \exp \left( -\frac{1}{2} \left( \Lambda_{11} - \Lambda_{22}^{-1} \Lambda_{12}^2 \right) (z_1 - \mu_1)^2 \right)
		\end{eqnarray}
		
		\item 従って、$q_1^*(z_1)$は次のようになる
		\begin{eqnarray}
			&& q_1^*(z_1) \nonumber \\
			&=& \frac{1}{2\pi} \frac{1}{|\bm{\Lambda}|^{-\frac{1}{2}}} \int \exp \left( -\frac{1}{2} (\bm{z} - \bm{\mu})^T \bm{\Lambda} (\bm{z} - \bm{\mu}) \right) dz_2 \\
			&=& \frac{1}{2\pi} \frac{1}{\left( \Lambda_{11} \Lambda_{22} - \Lambda_{12}^2 \right)^{-\frac{1}{2}}} \left( 2\pi \Lambda_{22}^{-1} \right)^\frac{1}{2} \nonumber \\
			&& \qquad \exp \left( -\frac{1}{2} \left( \Lambda_{11} - \Lambda_{22}^{-1} \Lambda_{12}^2 \right) (z_1 - \mu_1)^2 \right) \\
			&=& \frac{1}{(2\pi)^\frac{1}{2}} \frac{1}{(\Lambda_{11} - \Lambda_{22}^{-1} \Lambda_{12}^2)^{-\frac{1}{2}}} \nonumber \\
			&& \qquad \exp \left( -\frac{1}{2} \left( \Lambda_{11} - \Lambda_{22}^{-1} \Lambda_{12}^2 \right) (z_1 - \mu_1)^2 \right) \\
			&=& \color{red}\mathcal{N}(z_1 | \mu_1, (\Lambda_{11} - \Lambda_{22}^{-1} \Lambda_{12}^2)^{-1})\normalcolor
		\end{eqnarray}
		
		\item 共分散行列$\bm{\Sigma}$を、精度行列$\bm{\Lambda}$を使って次のように定めれば
		\begin{equation}
			\left( \begin{array}{ll} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22} \end{array} \right)^{-1} = \left( \begin{array}{ll} \Lambda_{11} & \Lambda_{12} \\ \Lambda_{21} & \Lambda_{22} \end{array} \right) \quad (\Sigma_{12} = \Sigma_{21})
		\end{equation}
		次が成り立つから
		\begin{equation}
			 \left( \begin{array}{ll} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22} \end{array} \right) \left( \begin{array}{ll} \Lambda_{11} & \Lambda_{12} \\ \Lambda_{21} & \Lambda_{22} \end{array} \right) = \bm{I}
		\end{equation}
		各成分に注目すれば
		\begin{eqnarray}
			\left\{ \begin{array}{l} \Sigma_{11} \Lambda_{11} + \Sigma_{12} \Lambda_{21} = 1 \\
			\Sigma_{11} \Lambda_{12} + \Sigma_{12} \Lambda_{22} = 0 \end{array} \right.
		\end{eqnarray}
		これを$\Sigma_{11}$について解けば
		\begin{eqnarray}
			&& \Sigma_{11} \Lambda_{11} + \left( -\Lambda_{22}^{-1} \Lambda_{12} \Sigma_{11} \right) \Lambda_{21} = 1 \\
			&\Rightarrow& \Sigma_{11} \left( \Lambda_{11} - \Lambda_{22}^{-1} \Lambda_{12} \Lambda_{21} \right) = 1 \\
			&\Rightarrow& \Sigma_{11} \left( \Lambda_{11} - \Lambda_{22}^{-1} \Lambda_{12}^2 \right) = 1 \\
			&\Rightarrow& \Sigma_{11} = \left( \Lambda_{11} - \Lambda_{22}^{-1} \Lambda_{12}^2 \right)^{-1}
		\end{eqnarray}
		
		\item これから、$q_1^*(z_1)$は次のようにも書ける
		\begin{eqnarray}
			&& q_1^*(z_1) \nonumber \\
			&=& \frac{1}{(2\pi)^\frac{1}{2}} \frac{1}{(\Lambda_{11} - \Lambda_{22}^{-1} \Lambda_{12}^2)^{-\frac{1}{2}}} \nonumber \\
			&& \qquad \exp \left( -\frac{1}{2} \left( \Lambda_{11} - \Lambda_{22}^{-1} \Lambda_{12}^2 \right) (z_1 - \mu_1)^2 \right) \\
			&=& \frac{1}{(2\pi)^\frac{1}{2}} \frac{1}{\Sigma_{11}^\frac{1}{2}} \exp \left( -\frac{1}{2} \Sigma_{11}^{-1} (z_1 - \mu_1)^2 \right) \\
			&=& \color{red}\mathcal{N}(z_1 | \mu_1, \Sigma_{11})\normalcolor
		\end{eqnarray}
		
		\item $q_2^*(z_2)$は、対称性から次のようになる
		\begin{equation}
			q_2^*(z_2) = \color{red}\mathcal{N}(z_2 | \mu_2, (\Lambda_{22} - \Lambda_{11}^{-1} \Lambda_{12}^2)^{-1})\normalcolor = \color{red}\mathcal{N}(z_2 | \mu_2, \Sigma_{22})\normalcolor
		\end{equation}
	\end{itemize} \
	
	\item 2つの解の比較
	\begin{itemize}
		\item $\KL(q || p)$の最小化によって次の解を得た
		\begin{eqnarray}
			q_1^*(z_1) &=& \color{red}\mathcal{N}(z_1 | \mu_1, \Lambda_{11}^{-1})\normalcolor \\
			q_2^*(z_2) &=& \color{red}\mathcal{N}(z_2 | \mu_2, \Lambda_{22}^{-1})\normalcolor
		\end{eqnarray}
		\newline
		
		\item $\KL(p || q)$の最小化では、次の解を得た
		\begin{eqnarray}
			q_1^*(z_1) &=& \color{red}\mathcal{N}(z_1 | \mu_1, \Sigma_{11})\normalcolor \\
			\Sigma_{11} &=& \left( \Lambda_{11} - \Lambda_{22}^{-1} \Lambda_{12}^2 \right)^{-1} \\
			q_2^*(z_2) &=& \color{red}\mathcal{N}(z_2 | \mu_2, \Sigma_{22})\normalcolor \\
			\Sigma_{22} &=& (\Lambda_{22} - \Lambda_{11}^{-1} \Lambda_{12}^2)^{-1}
		\end{eqnarray}
		
		\item $p(\bm{z}) = \mathcal{N}(\bm{z} | \bm{\mu}, \bm{\Sigma})$の平均は$\bm{\mu} = \left[ \mu_1, \mu_2 \right]^T$であったので、いずれの場合も、\alert{平均は正しく捉えている}
		\item しかし、両者の間では、\alert{分散が異なっている}
		\item また、変数$z_1$と$z_2$の間の\alert{相関は消えてなくなっている}
		\newline
		\item 両者の違いを、次の図\ref{fig:comparison-between-kl-divergence}に示す
		\newline
		\item \color{green}緑色\normalcolor の線が真の分布$p(\bm{z})$を表す
		\item 左側の\color{red}赤線\normalcolor は、$\KL(q || p)$の最小化によって得られた分布$q(\bm{z})$
		\item 右側の\color{red}赤線\normalcolor は、$\KL(p || q)$の最小化によって得られた分布$q(\bm{z})$
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{変分推論}

\begin{figure}[h]
	\centering
	\includegraphics[clip,scale=0.75,trim=1cm 14.5cm 1cm 2cm,page=488]{../pattern-recognition-and-machine-learning.pdf}
	\caption{KLダイバージェンスの2つの形の比較}
	\label{fig:comparison-between-kl-divergence}
\end{figure}

\end{frame}

\begin{frame}{変分推論}

\begin{itemize}
	\item 2つの解の比較
	\begin{itemize}
		\item $\KL(q || p)$の最小化で得られる$q(\bm{z})$は、\alert{分散が小さくなる方向に制御されていて}、それと直交する方向の分散は、大きく過小評価されている
		\item 分解による近似では、一般に事後分布$p(\bm{Z} | \bm{X})$を\alert{コンパクトに近似しすぎる}
		\newline
		\item $\KL(p || q)$の最小化で得られる$q(\bm{z})$は、\alert{分散が大きくなる方向に制御されていて}、それと直交する方向の分散は、過大評価されている
		\item 非常に低い確率しか持たないはずの領域にも、多くの確率質量が割り当てられている
		\newline
		\item 変分推論では、計算コストの観点から$\KL(q || p)$の方を用いる
		\item $\KL(q || p)$の計算には、$q$に関する期待値の評価が含まれるので、$q$を単純な形に制限することで、必要な期待値を単純化できる
		\item $\KL(p || q)$の計算には、真の事後分布$p$に関する期待値の計算が必要である
	\end{itemize} \
	
	\item 違いが生じる理由
	\begin{itemize}
		\item KLダイバージェンス$\KL(q || p)$は次のようであった
		\begin{equation}
			\KL(q || p) = -\int q(\bm{Z}) \ln \frac{p(\bm{Z} | \bm{X})}{q(\bm{Z})} d\bm{Z}
		\end{equation}
		
		\item $\KL(q || p)$が大きくなる主要因は、$p(\bm{Z})$がほとんど$0$で、$q(\bm{Z})$はそうでない領域
		\item 従って、$\KL(q || p)$を最小化すると、$q(\bm{Z})$は、\color{red}$p(\bm{Z})$が小さい領域を避けるようになる\normalcolor
		\newline
		\item また、$\KL(p || q)$は次のようであった
		\begin{equation}
			\KL(p || q) = -\int p(\bm{Z} | \bm{X}) \ln \frac{q(\bm{Z})}{p(\bm{Z} | \bm{X})} d\bm{Z}
		\end{equation}
		
		\item $\KL(p || q)$が大きくなる主要因は、$q(\bm{Z})$がほとんど$0$で、$p(\bm{Z})$はそうではない領域
		\item 従って、$\KL(p || q)$を最小化すると、$q(\bm{Z})$は、\color{red}$p(\bm{Z})$が$0$でない領域にも、必ず確率を持たせるようになる\normalcolor
		\newline
		\item 別の分布について、この両者の振舞いの違いを観察してみよう
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{変分推論}

\begin{itemize}
	\item 多峰性のある分布を、単峰の分布で近似する場合
	\begin{itemize}
		\item $\KL(q || p)$を最小化する変分近似では、\alert{多数ある峰のうちの1つ}を再現
		\item $\KL(p || q)$を最小化する変分近似では、\alert{全ての峰を平均したような分布}が得られる
		\newline
		\item 多峰性のある分布を平均してしまうと、\alert{予測性能の悪化}をもたらす
		\newline
		\item これらの比較を次の図\ref{fig:comparison-between-kl-divergence-2}に示す
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{変分推論}

\begin{figure}[h]
	\centering
	\includegraphics[clip,scale=0.7,trim=1cm 13cm 1cm 2cm,page=489]{../pattern-recognition-and-machine-learning.pdf}
	\caption{KLダイバージェンスの2つの形の別の比較}
	\label{fig:comparison-between-kl-divergence-2}
\end{figure}

\end{frame}

\begin{frame}{変分推論}

\begin{itemize}
	\item ここまでの話の流れ
	\begin{enumerate}
		\item 分解$q(\bm{Z}) = \prod_i q_i(\bm{Z}_i)$を使ったエビデンス下界$\mathcal{L}(q)$の最適化は、$\KL(q(\bm{Z}) || p(\bm{Z} | \bm{X}))$の最小化と等価である
		\newline
		\item $\KL(q || p)$と、$\KL(p || q)$を最小化する変分近似を、2変数のガウス分布を例として試した
		\newline
		\item $\KL(q || p)$の最小化を使って求めた$q(\bm{Z})$は、事後分布$p(\bm{Z} | \bm{X})$を\alert{コンパクトに近似}する傾向にあった
		\newline
		\item $\KL(p || q)$の最小化によって求めた$q(\bm{Z})$は、事後分布$p(\bm{Z} | \bm{X})$を\alert{大きく捉えて近似}する傾向にあった
	\end{enumerate} \
	
	\item これからの話の流れ
	\begin{itemize}
		\item 変分推論の具体的な例について更に見ていく
		\item 離散潜在変数のモデル(\alert{二値スパース符号化モデル})に変分推論を適用する
		\item 連続潜在変数の場合は、簡単な確率モデルを使って、変分推論を試してみる
	\end{itemize}
\end{itemize}

\end{frame}

\subsection{離散潜在変数の変分推論}

\begin{frame}{離散潜在変数の変分推論}

\begin{itemize}
	\item 離散潜在変数をもつ変分推論の概要
	\begin{itemize}
		\item ここでは単純な場合を扱う
		\item データ$\bm{x}_i$に対する潜在変数$\bm{z}_i$は、\alert{各要素が二値である}とする
		\item データ$\bm{x}$は$D$次元、潜在変数$\bm{z}$は$K$次元とする
		\newline
		\item 分布$q(\bm{z}_i | \bm{x}_i)$は、平均場近似によって、次のように分解できるとする
		\begin{equation}
			q(\bm{z}_i | \bm{x}_i) = \prod_k q(z_{ik} | \bm{x}_i)
		\end{equation}
		
		\item 潜在変数は二値であるから、\color{red}$q(z_{ik} = 1 | \bm{x}_i) = \widehat{z_{ik}}$\normalcolor と書くことにする
		\item このとき、$q(z_{ik} | \bm{x}_i)$は次のようになる
		\begin{equation}
			q(z_{ik} | \bm{x}_i) = \widehat{z_{ik}}^{z_{ik}} \left( 1 - \widehat{z_{ik}} \right)^{\left( 1 - z_{ik} \right)}
		\end{equation}
		
		\item 各パラメータ$\widehat{z_{ik}}$について、下界$\mathcal{L}(q)$を順番に最適化することを、$\mathcal{L}(q)$が収束するまで繰り返し行う
		\item 即ち、以下の不動点方程式を、各$\widehat{z_{ik}}$について繰り返し解く
		\begin{equation}
			\frac{\partial}{\partial \widehat{z_{ik}}} \mathcal{L}(q) = 0
		\end{equation}
		
		\item 離散潜在変数の場合は、単なる標準的な最適化問題を解くことになる
	\end{itemize} \
	
	\item 二値スパース符号化モデル
	\begin{itemize}
		\item 二値スパース符号化モデルでのデータ生成過程は、次のようになる
		\newline
		
		\item 各データ$\bm{x}_i$には、潜在変数$\bm{z}_i \in \left\{ 0, 1 \right\}^K$が対応する
		\item $\bm{z}_i$に対し、重み$\bm{W}$を用いて\alert{線形変換}を施し、更に\alert{ガウスノイズ}を足し合わせることで、データ$\bm{x}_i$が生成される
		\newline
		
		\item 潜在変数$\bm{z}_i$は、$K$次元ベクトルであり、その\color{red}各要素は$0$または$1$\normalcolor である
		\newline
		
		\item 従って、確率分布は次のようになる
		\begin{eqnarray}
			p(z_{ik} = 1) &=& \sigma(b_{ik}) \\
			p(\bm{x}_i | \bm{z}_i) &=& \mathcal{N}(\bm{x}_i | \bm{W} \bm{z}_i, \bm{\beta}^{-1})
		\end{eqnarray}
		
		\item $\sigma(\cdot)$はシグモイド関数、$\bm{b}_i = \left[ b_{i1}, \ldots, b_{iK} \right]^T$は学習可能な\alert{バイアス}、$\bm{W}$は学習可能な\alert{重み行列}、$\bm{\beta}$は学習可能な\alert{対角精度行列}である
		\item $\bm{\beta} = \diag \left( \beta_1, \beta_2, \ldots, \beta_D \right)$と書ける
		\newline
		
		\item $p(\bm{z}_i)$は次のように計算できる
		\begin{eqnarray}
			p(\bm{z}_i) &=& \prod_k p(z_{ik}) \\
			&=& \prod_k \left( \sigma(b_{ik}) \right)^{z_{ik}} \left( 1 - \sigma(b_{ik}) \right)^{1 - z_{ik}} \\
			&=& \prod_k \left( \sigma(b_{ik}) \right)^{z_{ik}} \left( \sigma(-b_{ik}) \right)^{1 - z_{ik}}
		\end{eqnarray}
		
		\item 事後分布$p(\bm{z}_i | \bm{x}_i)$は\alert{複雑}であり、表現することも、計算することもできない
		\item 従って、最尤推定の手法(EMアルゴリズム)により学習することはできない
		\newline
		
		\item 例えばバイアス$b_{ik}$に関する微分を考えると
		\begin{eqnarray}
			&& \frac{\partial}{\partial b_{ik}} \ln p(\bm{z}_i | \bm{x}_i) \nonumber \\
			&=& \frac{\partial}{\partial b_{ik}} \ln \frac{p(\bm{x}_i, \bm{z}_i)}{p(\bm{x}_i)} \\
			&=& \frac{\partial}{\partial b_{ik}} \left( \ln p(\bm{x}_i, \bm{z}_i) - \ln p(\bm{x}_i) \right)
		\end{eqnarray}
		であって、第1項の$p(\bm{x}_i, \bm{z}_i)$は容易に計算できるが、第2項について考えると
		\begin{eqnarray}
			&& \frac{\partial}{\partial b_{ik}} \ln p(\bm{x}_i) \nonumber \\
			&=& \frac{1}{p(\bm{x}_i)} \frac{\partial}{\partial b_{ik}} p(\bm{x}_i) \\
			&=& \frac{1}{p(\bm{x}_i)} \frac{\partial}{\partial b_{ik}} \sum_{\bm{z}_i} p(\bm{x}_i, \bm{z}_i) \\
			&=& \frac{1}{p(\bm{x}_i)} \frac{\partial}{\partial b_{ik}} \sum_{\bm{z}_i} p(\bm{z}_i) p(\bm{x}_i | \bm{z}_i) \\
			&=& \frac{1}{p(\bm{x}_i)} \sum_{\bm{z}_i} p(\bm{x}_i | \bm{z}_i) \frac{\partial}{\partial b_{ik}} p(\bm{z}_i) \\
			&=& \sum_{\bm{z}_i} \frac{1}{p(\bm{x}_i)} \frac{p(\bm{x}_i, \bm{z}_i)}{p(\bm{z}_i)} \frac{\partial}{\partial b_{ik}} p(\bm{z}_i) \\
			&=& \sum_{\bm{z}_i} p(\bm{z}_i | \bm{x}_i) \frac{1}{p(\bm{z}_i)} \frac{\partial}{\partial b_{ik}} p(\bm{z}_i) \\
			&=& \sum_{\bm{z}_i} p(\bm{z}_i | \bm{x}_i) \frac{\partial}{\partial b_{ik}} \ln p(\bm{z}_i) \\
			&=& \mathbb{E}_{\bm{z}_i \sim p(\bm{z}_i | \bm{x}_i)} \left[ \frac{\partial}{\partial b_{ik}} \ln p(\bm{z}_i) \right]
		\end{eqnarray}
		何とか$p(\bm{z}_i)$と$p(\bm{x}_i | \bm{z}_i)$を使って計算できないかと試行錯誤したが、結局、\color{red}$p(\bm{z}_i | \bm{x}_i)$に関する期待値の計算が必要\normalcolor になった
		\newline
		
		\item $p(\bm{x}_i, \bm{z}_i)$と、$p(\bm{z}_i | \bm{x}_i)$のグラフ構造を次の図\ref{fig:graph-structure-of-binary-sparse-coding-model}に示す
		\item 図では、$\bm{h} = \bm{z}$、$\bm{v} = \bm{x}$のように読み替える必要がある
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{離散潜在変数の変分推論}

\begin{figure}[h]
	\centering
	\includegraphics[clip,scale=0.6,trim=2cm 20cm 2cm 2cm,page=656]{../goodfellow-deep-learning.pdf}
	\caption{二値スパース符号化モデルのグラフ構造(4つの潜在変数をもつ場合)}
	\label{fig:graph-structure-of-binary-sparse-coding-model}
\end{figure}

\end{frame}

\begin{frame}{離散潜在変数の変分推論}

\begin{itemize}
	\item 変分推論による二値スパース符号化モデルの学習
	\begin{itemize}
		\item 最尤推定の手法を諦める代わりに、変分推論を使うことで、困難を解決できる
		\item 分布$p(\bm{z}_i | \bm{x}_i)$を$q(\bm{z}_i)$で表現し、更に平均場近似を行う
		\item 但し$\bm{z}_i = \left[ z_{i1}, z_{i2}, \ldots, z_{iK} \right]^T$とする
		\begin{equation}
			q(\bm{z}_i | \bm{x}_i) = \prod_k q(z_{ik} | \bm{x}_i)
		\end{equation}
		
		\item 潜在変数の各要素は二値であるから、各$q(z_{ik} | \bm{x}_i)$は\alert{ベルヌーイ分布}とすればよい
		\item 即ち、\color{red}$q(z_{ik} = 1 | \bm{x}_i) = \widehat{z_{ik}}$\normalcolor とする
		\item $\widehat{z_{ik}} \neq 0, 1$という制約を課すことで、$\ln \widehat{z_{ik}}$を計算できる
		\newline
		\item このようにすれば、潜在変数$z_{ik}, z_{il} (k \neq l)$間の\alert{相関を断ち切る}ことができる
		\item 先程の図\ref{fig:graph-structure-of-binary-sparse-coding-model}の右側から、潜在変数間の辺を、全て消し去ることになる
		
		\item これで、平均場近似により、因数分解可能な$q$を表現することができる
		\begin{eqnarray}
			q(\bm{z}_i | \bm{x}_i) &=& \prod_k q(z_{ik} | \bm{x}_i) \\
			&=& \prod_k \widehat{z_{ik}}^{z_{ik}} \left( 1 - \widehat{z_{ik}} \right)^{1 - z_{ik}}
		\end{eqnarray}
		\begin{eqnarray}
			q(\bm{Z} | \bm{X}) &=& \prod_i q(\bm{z}_i | \bm{x}_i) \\
			&=& \prod_i \prod_k \widehat{z_{ik}}^{z_{ik}} \left( 1 - \widehat{z_{ik}} \right)^{1 - z_{ik}}
		\end{eqnarray}
		
		\item ソフトウェア上では、丸め誤差などによって$\widehat{z_{ik}}$が$0$や$1$になり、計算を続行できなくなるかもしれない
		\item これを回避するためには、パラメータ$\widetilde{\bm{z}}_i$を使って二値スパース符号化モデルを学習させる
		\item そして、$\widehat{\bm{z}}_i = \sigma(\widetilde{\bm{z}}_i)$の関係によって、$\widehat{\bm{z}}_i = \left[ \widehat{z_{i1}}, \ldots, \widehat{z_{iK}} \right]^T$を得るようにする
		\item $\ln \widehat{z_{ik}} = \ln \sigma(\widetilde{z_{ik}}) = -\zeta(-\widetilde{z_{ik}})$によって、コンピュータ上で安全に$\ln \widehat{z_{ik}}$を計算できる ($\zeta(\cdot)$はソフトプラス関数)
		\newline
		
		\item 変分推論のために、まずはエビデンス下界$\mathcal{L}(q)$を計算する
		\begin{eqnarray}
			&& \mathcal{L}(q) \nonumber \\
			&=& \sum_{\bm{Z}} q(\bm{Z} | \bm{X}) \ln \frac{p(\bm{X}, \bm{Z})}{q(\bm{Z} | \bm{X})} \\
			&=& \sum_{\bm{Z}} q(\bm{Z} | \bm{X}) \ln \frac{p(\bm{Z}) p(\bm{X} | \bm{Z})}{q(\bm{Z} | \bm{X})} \\
			&=& \sum_{\bm{Z}} q(\bm{Z} | \bm{X}) \left( \ln p(\bm{Z}) + \ln p(\bm{X} | \bm{Z}) - \ln q(\bm{Z} | \bm{X}) \right) \\
			&=& \sum_{\bm{Z}} q(\bm{Z} | \bm{X}) \left( \ln p(\bm{Z}) - \ln q(\bm{Z} | \bm{X}) \right) + \nonumber \\
			&& \qquad \sum_{\bm{Z}} q(\bm{Z} | \bm{X}) \ln p(\bm{X} | \bm{Z})
		\end{eqnarray}
		
		\item $q(\bm{Z} | \bm{X})$についての期待値を取っていることに注意する
		\item 全ての$\bm{Z}$についての和を取ればよいので、第1項は次のようになる
		\begin{eqnarray}
			&& \sum_{\bm{Z}} q(\bm{Z} | \bm{X}) \left( \ln p(\bm{Z}) - \ln q(\bm{Z} | \bm{X}) \right) \nonumber \\
			&=& \sum_i \sum_k \sum_{z_{ik}} q(z_{ik} | \bm{x}_i) \left( \ln p(z_{ik}) - \ln q(z_{ik} | \bm{x}_i) \right) \\
			&=& \sum_i \sum_k q(z_{ik} = 1 | \bm{x}_i) \left( \ln p(z_{ik} = 1) - \ln q(z_{ik} = 1 | \bm{x}_i) \right) + \nonumber \\
			&& \qquad q(z_{ik} = 0 | \bm{x}_i) \left( \ln p(z_{ik} = 0) - \ln q(z_{ik} = 0 | \bm{x}_i) \right) \\
			&=& \sum_i \sum_k \left\{ \widehat{z_{ik}} \left( \ln \sigma(b_{ik}) - \ln \widehat{z_{ik}} \right) + \right. \nonumber \\
			&& \qquad \left. \left( 1 - \widehat{z_{ik}} \right) \left( \ln \left( 1 - \sigma(b_{ik}) \right) - \ln \left( 1 - \widehat{z_{ik}} \right) \right) \right\} \\
			&=& \sum_i \sum_k \left\{ \widehat{z_{ik}} \left( \ln \sigma(b_{ik}) - \ln \widehat{z_{ik}} \right) + \right. \nonumber \\
			&& \qquad \left. \left( 1 - \widehat{z_{ik}} \right) \left( \ln \sigma(-b_{ik}) - \ln \left( 1 - \widehat{z_{ik}} \right) \right) \right\}
		\end{eqnarray}
		
		\item また第2項は、次のようになる
		\begin{eqnarray}
			&& \sum_{\bm{Z}} q(\bm{Z} | \bm{X}) \ln p(\bm{X} | \bm{Z}) \nonumber \\
			&=& \sum_i \sum_{\bm{z}_i} q(\bm{z}_i | \bm{x}_i) \ln p(\bm{x}_i | \bm{z}_i) \\
			&=& \sum_i \sum_{\bm{z}_i} q(\bm{z}_i | \bm{x}_i) \ln \mathcal{N}(\bm{x}_i | \bm{W} \bm{z}_i, \bm{\beta}^{-1})
		\end{eqnarray}
		
		\item 但し、$\ln \mathcal{N}(\bm{x}_i | \bm{W} \bm{z}_i, \bm{\beta}^{-1})$は次のように分解できる
		\begin{eqnarray}
			&& \ln \mathcal{N}(\bm{x}_i | \bm{W} \bm{z}_i, \bm{\beta}^{-1}) \nonumber \\
			&=& \ln \Bigg( \frac{1}{(2\pi)^\frac{D}{2}} \frac{1}{|\bm{\beta}^{-1}|^\frac{1}{2}} \nonumber \\
			&& \qquad \exp \left( -\frac{1}{2} \left( \bm{x}_i - \bm{W} \bm{z}_i \right)^T \bm{\beta} \left( \bm{x}_i - \bm{W} \bm{z}_i \right) \right) \Bigg) \\
			&=& -\frac{D}{2} \ln 2\pi - \frac{1}{2} \ln |\bm{\beta}^{-1}| - \nonumber \\
			&& \qquad \frac{1}{2} \left( \bm{x}_i - \bm{W} \bm{z}_i \right)^T \bm{\beta} \left( \bm{x}_i - \bm{W} \bm{z}_i \right)
		\end{eqnarray}
		
		\item ここで、$\bm{\beta} = \diag \left( \beta_1, \beta_2, \ldots, \beta_D \right)$であるので
		\begin{eqnarray}
			\ln |\bm{\beta}^{-1}| &=& \ln |\bm{\beta}|^{-1} = - \ln |\bm{\beta}| \nonumber \\
			&=& - \ln \prod_{j = 1}^D \beta_j = - \sum_{j = 1}^D \ln \beta_j
		\end{eqnarray}
		となるほか
		\begin{equation}
			\left( \bm{x}_i - \bm{W} \bm{z}_i \right)^T \bm{\beta} \left( \bm{x}_i - \bm{W} \bm{z}_i \right) = \sum_{j = 1}^D \beta_j \left( x_{ij} - \bm{W}_{j:} \bm{z}_i \right)^2
		\end{equation}
		であるから
		\begin{eqnarray}
			&& \ln \mathcal{N}(\bm{x}_i | \bm{W} \bm{z}_i, \bm{\beta}^{-1}) \nonumber \\
			&=& -\frac{1}{2} \sum_j \ln 2\pi + \frac{1}{2} \sum_j \ln \beta_j - \frac{1}{2} \sum_j \beta_j \left( x_{ij} - \bm{W}_{j:} \bm{z}_i \right)^2 \\
			&=& \frac{1}{2} \sum_j \left( -\ln 2\pi + \ln \beta_j - \beta_j \left( x_{ij} - \bm{W}_{j:} \bm{z}_i \right)^2 \right) \\
			&=& \frac{1}{2} \sum_j \left( \ln \frac{\beta_j}{2\pi} - \beta_j \left( x_{ij} - \bm{W}_{j:} \bm{z}_i \right)^2 \right) \\
			&=& \frac{1}{2} \sum_j \left( \ln \frac{\beta_j}{2\pi} - \beta_j \left( x_{ij} - \sum_k W_{jk} z_{ik} \right)^2 \right)
		\end{eqnarray}
		
		\item 上式の対数を外すと、確率分布の積になっていることが分かる
		\begin{eqnarray}
			&& \mathcal{N}(\bm{x}_i | \bm{W} \bm{z}_i, \bm{\beta}^{-1}) \nonumber \\
			&=& \exp \left( \sum_j \left( \frac{1}{2} \ln \frac{\beta_j}{2\pi} - \frac{1}{2} \beta_j \left( x_{ij} - \bm{W}_{j:} \bm{z}_i \right)^2 \right) \right) \\
			&=& \prod_j \exp \left( \frac{1}{2} \ln \frac{\beta_j}{2\pi} - \frac{1}{2} \beta_j \left( x_{ij} - \bm{W}_{j:} \bm{z}_i \right)^2 \right) \\
			&=& \prod_j \exp \left( \frac{1}{2} \ln \frac{\beta_j}{2\pi} \right) \exp \left( - \frac{1}{2} \beta_j \left( x_{ij} - \bm{W}_{j:} \bm{z}_i \right)^2 \right) \\
			&=& \prod_j \left( \frac{\beta_j}{2\pi} \right)^\frac{1}{2} \exp \left( - \frac{1}{2} \beta_j \left( x_{ij} - \bm{W}_{j:} \bm{z}_i \right)^2 \right) \\
			&=& \prod_j \left( \frac{1}{(2\pi \beta_j^{-1})^\frac{1}{2}} \exp \left( - \frac{1}{2} \beta_j \left( x_{ij} - \bm{W}_{j:} \bm{z}_i \right)^2 \right) \right) \\
			&=& \prod_j \mathcal{N}(x_{ij} | \bm{W}_{j:} \bm{z}_i, \beta_j^{-1})
		\end{eqnarray}
		
		\item 精度行列$\bm{\beta}$は対角行列であるから、$\bm{x}_i$の各成分は\alert{無相関}である
		\item 従って、$\mathcal{N}(\bm{x}_i | \bm{W} \bm{z}_i, \bm{\beta}^{-1})$は、各成分$x_{ij}$についての確率$\mathcal{N}(x_{ij} | \bm{W}_{j:} \bm{z}_i, \beta_j^{-1})$の積として、記述できる
		\newline
		
		\item さて、第2項は
		\begin{eqnarray}
			&& \sum_{\bm{Z}} q(\bm{Z} | \bm{X}) \ln p(\bm{X} | \bm{Z}) \nonumber \\
			&=& \sum_i \sum_{\bm{z}_i} q(\bm{z}_i | \bm{x}_i) \ln \mathcal{N}(\bm{x}_i | \bm{W} \bm{z}_i, \bm{\beta}^{-1}) \\
			&=& \sum_i \mathbb{E}_{\bm{z}_i \sim q(\bm{z}_i | \bm{x}_i)} \left[ \ln \mathcal{N}(\bm{x}_i | \bm{W} \bm{z}_i, \bm{\beta}^{-1}) \right] \\
			&=& \sum_i \mathbb{E}_{\bm{z}_i} \left[ \frac{1}{2} \sum_j \left( \ln \frac{\beta_j}{2\pi} - \beta_j \left( x_{ij} - \sum_k W_{jk} z_{ik} \right)^2 \right) \right] \\
			&=& \frac{1}{2} \sum_i \sum_j \mathbb{E}_{\bm{z}_i} \left[ \ln \frac{\beta_j}{2\pi} - \beta_j \left( x_{ij} - \sum_k W_{jk} z_{ik} \right)^2 \right] \\
			&=& \frac{1}{2} \sum_i \sum_j \left( \ln \frac{\beta_j}{2\pi} - \beta_j \mathbb{E}_{\bm{z}_i} \left[ \left( x_{ij} - \sum_k W_{jk} z_{ik} \right)^2 \right] \right)
		\end{eqnarray}
		ここで
		\begin{eqnarray}
			&& \mathbb{E}_{\bm{z}_i} \left[ \left( x_{ij} - \sum_k W_{jk} z_{ik} \right)^2 \right] \nonumber \\
			&=& \mathbb{E}_{\bm{z}_i} \left[ x_{ij}^2 - 2 x_{ij} \sum_k W_{jk} z_{ik} + \left( \sum_k W_{jk} z_{ik} \right)^2 \right] \\
			&=& x_{ij}^2 - 2 x_{ij} \mathbb{E}_{\bm{z}_i} \left[ \sum_k W_{jk} z_{ik} \right] + \mathbb{E}_{\bm{z}_i} \left[ \left( \sum_k W_{jk} z_{ik} \right)^2 \right]
		\end{eqnarray}
		各項を順番に計算すると
		\begin{eqnarray}
			&& \mathbb{E}_{\bm{z}_i} \left[ \sum_k W_{jk} z_{ik} \right] \nonumber \\
			&=& \sum_k W_{jk} \mathbb{E}_{\bm{z}_i} \left[ z_{ik} \right] \\
			&=& \sum_k W_{jk} \mathbb{E}_{z_{ik}} \left[ z_{ik} \right] \\
			&=& \sum_k W_{jk} \left( q(z_{ik} = 1 | \bm{x}_i) \cdot 1 + q(z_{ik} = 0 | \bm{x}_i) \cdot 0 \right) \\
			&=& \sum_k W_{jk} \widehat{z_{ik}}
		\end{eqnarray}
		また
		\begin{eqnarray}
			&& \mathbb{E}_{\bm{z}_i} \left[ \left( \sum_k W_{jk} z_{ik} \right)^2 \right] \nonumber \\
			&=& \mathbb{E}_{\bm{z}_i} \left[ \sum_k \sum_l W_{jk} W_{jl} z_{ik} z_{il} \right] \\
			&=& \mathbb{E}_{\bm{z}_i} \left[ \sum_k \left( W_{jk}^2 z_{ik}^2 + \sum_{l \neq k} \left( W_{jk} W_{jl} z_{ik} z_{il} \right) \right) \right] \\
			&=& \sum_k \left( W_{jk}^2 \mathbb{E}_{\bm{z}_i} \left[ z_{ik}^2 \right] + \sum_{l \neq k} W_{jk} W_{jl} \mathbb{E}_{\bm{z}_i} \left[ z_{ik} z_{il} \right] \right)
		\end{eqnarray}
		\begin{eqnarray}
			&& \mathbb{E}_{\bm{z}_i} \left[ z_{ik}^2 \right] \nonumber \\
			&=& \mathbb{E}_{z_{ik}} \left[ z_{ik}^2 \right] \\
			&=& q(z_{ik} = 1 | \bm{x}_i) \cdot 1^2 + q(z_{ik} = 0 | \bm{x}_i) \cdot 0^2 \\
			&=& \widehat{z_{ik}}
		\end{eqnarray}
		\begin{eqnarray}
			&& \mathbb{E}_{\bm{z}_i} \left[ z_{ik} z_{il} \right] \nonumber \\
			&=& \mathbb{E}_{\bm{z}_i} \left[ z_{ik} \right] \mathbb{E}_{\bm{z}_i} \left[ z_{il} \right] \quad (\because \text{$z_{ik}$と$z_{il}$は独立であるため}) \\
			&=& \mathbb{E}_{z_{ik}} \left[ z_{ik} \right] \mathbb{E}_{z_{il}} \left[ z_{il} \right] \\
			&=& \widehat{z_{ik}} \widehat{z_{il}}
		\end{eqnarray}
		従って
		\begin{eqnarray}
			&& \mathbb{E}_{\bm{z}_i} \left[ \left( x_{ij} - \sum_k W_{jk} z_{ik} \right)^2 \right] \nonumber \\
			&=& x_{ij}^2 - 2 x_{ij} \mathbb{E}_{\bm{z}_i} \left[ \sum_k W_{jk} z_{ik} \right] + \mathbb{E}_{\bm{z}_i} \left[ \left( \sum_k W_{jk} z_{ik} \right)^2 \right] \\
			&=& x_{ij}^2 - 2 x_{ij} \sum_k W_{jk} \widehat{z_{ik}} + \nonumber \\
			&& \qquad \sum_k \left( W_{jk}^2 \widehat{z_{ik}} + \sum_{l \neq k} W_{jk} W_{jl} \widehat{z_{ik}} \widehat{z_{il}} \right)
		\end{eqnarray}
		これより
		\begin{eqnarray}
			&& \sum_{\bm{Z}} q(\bm{Z} | \bm{X}) \ln p(\bm{X} | \bm{Z}) \nonumber \\
			&=& \frac{1}{2} \sum_i \sum_j \Bigg( \ln \frac{\beta_j}{2\pi} - \beta_j \mathbb{E}_{\bm{z}_i} \left[ \left( x_{ij} - \sum_k W_{jk} z_{ik} \right)^2 \right] \Bigg) \\
			&=& \frac{1}{2} \sum_i \sum_j \Bigg( \ln \frac{\beta_j}{2\pi} - \beta_j \left( x_{ij}^2 - 2 x_{ij} \sum_k W_{jk} \widehat{z_{ik}} + \right. \nonumber \\
			&& \qquad \left. \sum_k \left( W_{jk}^2 \widehat{z_{ik}} + \sum_{l \neq k} W_{jk} W_{jl} \widehat{z_{ik}} \widehat{z_{il}} \right) \right) \Bigg)
		\end{eqnarray}
		
		\item よって、エビデンス下界$\mathcal{L}(q)$は
		\begin{eqnarray}
			&& \mathcal{L}(q) \nonumber \\
			&=& \sum_{\bm{Z}} q(\bm{Z} | \bm{X}) \left( \ln p(\bm{Z}) - \ln q(\bm{Z} | \bm{X}) \right) + \nonumber \\
			&& \qquad \sum_{\bm{Z}} q(\bm{Z} | \bm{X}) \ln p(\bm{X} | \bm{Z}) \\
			&=& \sum_i \sum_k \left\{ \widehat{z_{ik}} \left( \ln \sigma(b_{ik}) - \ln \widehat{z_{ik}} \right) + \right. \nonumber \\
			&& \qquad \left. \left( 1 - \widehat{z_{ik}} \right) \left( \ln \sigma(-b_{ik}) - \ln \left( 1 - \widehat{z_{ik}} \right) \right) \right\} + \nonumber \\
			&& \qquad \frac{1}{2} \sum_i \sum_j \Bigg( \ln \frac{\beta_j}{2\pi} - \beta_j \left( x_{ij}^2 - 2 x_{ij} \sum_k W_{jk} \widehat{z_{ik}} + \right. \nonumber \\
			&& \qquad \left. \sum_k \left( W_{jk}^2 \widehat{z_{ik}} + \sum_{l \neq k} W_{jk} W_{jl} \widehat{z_{ik}} \widehat{z_{il}} \right) \right) \Bigg)
		\end{eqnarray}
		
		\item この式には\alert{あまり美的魅力がない}(Somewhat unappealing aesthetically)が、事後分布よりは計算しやすい
		\item 事後分布$\ln p(\bm{Z} | \bm{X})$を最大化する代わりに、上記の下界$\mathcal{L}(q)$を$q$について最大化することができる
		\newline
		\item データ$\bm{x}_i$に対するパラメータ$\left\{ \widehat{z_{i1}}, \ldots, \widehat{z_{iK}} \right\}$をまとめて、\color{red}ベクトル$\widehat{\bm{z}}_i$として記述\normalcolor する
		\item パラメータ$\widehat{\bm{z}}_i = \left[ \widehat{z_{i1}}, \ldots, \widehat{z_{iK}} \right]^T$は、データ$\bm{x}_i$に対応する潜在変数$\bm{z}$の各要素が、$1$となる確率を集めたベクトルである
		\newline
		\item $\widehat{z_{ik}} = q(z_{ik} = 1 | \bm{x}_i)$と定義されていることに注意
		\newline
		\item よってベクトル$\widehat{\bm{z}}_i$は、データ$\bm{x}_i$に対応する\alert{二値のスパース符号}である
	\end{itemize} \
	
	\item 勾配上昇法を利用しない理由
	\begin{itemize}
		\item データ$\bm{X}$と、潜在変数$\bm{Z}$についての勾配上昇法を用いれば、学習することが可能
		\item 但し、その方法では、\color{red}各$\bm{x}_i$について\alert{平均場パラメータ}$\widehat{\bm{z}}_i$を保管する必要\normalcolor がある
		\item \alert{各事例について、動的に更新されるベクトルが必要}であるため、そのアルゴリズムを、数十億もの事例に対して適用することは困難である
		\newline
		\item また、\alert{収束するまで繰り返し計算を行う}ため、データ$\bm{x}$から、パラメータ$\widehat{\bm{z}}_i$を素早く抽出することができない
		\item 現実にデプロイされるときは、$\widehat{\bm{z}}_i$をリアルタイムで計算できなければならない
	\end{itemize} \
	
	\item \alert{不動点方程式}による平均場パラメータ$\widehat{\bm{z}}_i$の推定
	\begin{itemize}
		\item 勾配上昇法の代わりに、不動点方程式を使ってパラメータ$\widehat{z_{ik}}$を素早く推定できる
		\newline
		\item $\nabla_{\widehat{\bm{z}}_i} \mathcal{L}(q) = 0$をみたす、$\widehat{\bm{z}}_i$の極大値を見つけ出す
		\item $\widehat{\bm{z}}_i$の全ての成分について同時に解くことはできないので、各成分$\widehat{z_{ik}}$について繰り返し解く
		\newline
		
		\item 即ち、各パラメータ$\widehat{z_{ik}}$について、下界$\mathcal{L}(q)$を順番に最適化する手続きを、\color{red}$\mathcal{L}(q)$の収束基準を満たすまで繰り返す\normalcolor
		\begin{equation}
			\frac{\partial}{\partial \widehat{z_{ik}}} \mathcal{L}(q) = 0
		\end{equation}
		
		\item 平均場不動点方程式を導くためには、$\mathcal{L}(q)$を$\widehat{z_{ik}}$で微分する必要がある
		\begin{eqnarray}
			&& \frac{\partial}{\partial \widehat{z_{ik}}} \mathcal{L}(q) \nonumber \\
			&=& \frac{\partial}{\partial \widehat{z_{ik}}} \sum_i \sum_k \left\{ \widehat{z_{ik}} \left( \ln \sigma(b_{ik}) - \ln \widehat{z_{ik}} \right) + \right. \nonumber \\
			&& \qquad \left. \left( 1 - \widehat{z_{ik}} \right) \left( \ln \sigma(-b_{ik}) - \ln \left( 1 - \widehat{z_{ik}} \right) \right) \right\} + \nonumber \\
			&& \qquad \frac{\partial}{\partial \widehat{z_{ik}}} \frac{1}{2} \sum_i \sum_j \Bigg( \ln \frac{\beta_j}{2\pi} - \beta_j \left( x_{ij}^2 - 2 x_{ij} \sum_k W_{jk} \widehat{z_{ik}} + \right. \nonumber \\
			&& \qquad \left. \sum_k \left( W_{jk}^2 \widehat{z_{ik}} + \sum_{l \neq k} W_{jk} W_{jl} \widehat{z_{ik}} \widehat{z_{il}} \right) \right) \Bigg)
		\end{eqnarray}
		
		\item 前半部分は
		\begin{eqnarray}
			&& \frac{\partial}{\partial \widehat{z_{ik}}} \sum_i \sum_k \left\{ \widehat{z_{ik}} \left( \ln \sigma(b_{ik}) - \ln \widehat{z_{ik}} \right) + \right. \nonumber \\
			&& \qquad \left. \left( 1 - \widehat{z_{ik}} \right) \left( \ln \sigma(-b_{ik}) - \ln \left( 1 - \widehat{z_{ik}} \right) \right) \right\} \\
			&=& \frac{\partial}{\partial \widehat{z_{ik}}} \left\{ \widehat{z_{ik}} \left( \ln \sigma(b_{ik}) - \ln \widehat{z_{ik}} \right) + \right. \nonumber \\
			&& \qquad \left. \left( 1 - \widehat{z_{ik}} \right) \left( \ln \sigma(-b_{ik}) - \ln \left( 1 - \widehat{z_{ik}} \right) \right) \right\} \\
			&=& \frac{\partial}{\partial \widehat{z_{ik}}} \widehat{z_{ik}} \left( \ln \sigma(b_{ik}) - \ln \widehat{z_{ik}} \right) + \nonumber \\
			&& \qquad \frac{\partial}{\partial \widehat{z_{ik}}} \left( 1 - \widehat{z_{ik}} \right) \left( \ln \sigma(-b_{ik}) - \ln \left( 1 - \widehat{z_{ik}} \right) \right) \\
			&=& \left( \ln \sigma(b_{ik}) - \ln \widehat{z_{ik}} \right) + \widehat{z_{ik}} \left( -\frac{1}{\widehat{z_{ik}}} \right) + \nonumber \\
			&& \qquad \left( - \left( \ln \sigma(-b_{ik}) - \ln \left( 1 - \widehat{z_{ik}} \right) \right) \right) + \nonumber \\
			&& \qquad \left( 1 - \widehat{z_{ik}} \right) \frac{1}{1 - \widehat{z_{ik}}} \\
			&=& \ln \sigma(b_{ik}) - \ln \widehat{z_{ik}} - 1 - \ln \sigma(-b_{ik}) + \ln (1 - \widehat{z_{ik}}) + 1 \\
			&=& \ln \sigma(b_{ik}) - \ln \widehat{z_{ik}} - \ln \sigma(-b_{ik}) + \ln (1 - \widehat{z_{ik}}) \\
			&=& - \zeta(-b_{ik}) - \ln \widehat{z_{ik}} + \zeta(b_{ik}) + \ln (1 - \widehat{z_{ik}}) \\
			&=& \left( \zeta(b_{ik}) - \zeta(-b_{ik}) \right) - \ln \widehat{z_{ik}} + \ln (1 - \widehat{z_{ik}}) \\
			&=& b_{ik} - \ln \widehat{z_{ik}} + \ln (1 - \widehat{z_{ik}})
		\end{eqnarray}
		
		\item ここで、シグモイド関数$\sigma(\cdot)$と、ソフトプラス関数$\zeta(\cdot)$に関する、以下の公式を用いた
		\begin{eqnarray}
			&& \ln \sigma(x) = -\zeta(-x) \\
			&& \zeta(x) - \zeta(-x) = x
		\end{eqnarray}
		
		\item 後半部分は
		\begin{eqnarray}
			&& \frac{\partial}{\partial \widehat{z_{ik}}} \frac{1}{2} \sum_i \sum_j \Bigg( \ln \frac{\beta_j}{2\pi} - \beta_j \left( x_{ij}^2 - 2 x_{ij} \sum_k W_{jk} \widehat{z_{ik}} + \right. \nonumber \\
			&& \qquad \left. \sum_k \left( W_{jk}^2 \widehat{z_{ik}} + \sum_{l \neq k} W_{jk} W_{jl} \widehat{z_{ik}} \widehat{z_{il}} \right) \right) \Bigg) \\
			&=& \frac{1}{2} \sum_j \beta_j \frac{\partial}{\partial \widehat{z_{ik}}} \left( 2 x_{ij} \sum_k W_{jk} \widehat{z_{ik}} - \right. \nonumber \\
			&& \qquad \left. \sum_k \left( W_{jk}^2 \widehat{z_{ik}} + \sum_{l \neq k} W_{jk} W_{jl} \widehat{z_{ik}} \widehat{z_{il}} \right) \right) \\
			&=& \frac{1}{2} \sum_j \beta_j \frac{\partial}{\partial \widehat{z_{ik}}} \left( 2 x_{ij} \sum_k W_{jk} \widehat{z_{ik}} - \right. \nonumber \\
			&& \qquad \left( W_{jk}^2 \widehat{z_{ik}} + \sum_{l \neq k} W_{jk} W_{jl} \widehat{z_{ik}} \widehat{z_{il}} \right) - \nonumber \\
			&& \qquad \left. \sum_{m \neq k} \left( W_{jm}^2 \widehat{z_{im}} + \sum_{l \neq m} W_{jm} W_{jl} \widehat{z_{im}} \widehat{z_{il}} \right) \right) \\
			&=& \frac{1}{2} \sum_j \beta_j \left( 2 x_{ij} W_{jk} - W_{jk}^2 - \right. \nonumber \\
			&& \qquad \left. \sum_{l \neq k} W_{jk} W_{jl} \widehat{z_{il}} - \sum_{m \neq k} W_{jm} W_{jk} \widehat{z_{im}} \right) \\
			&=& \frac{1}{2} \sum_j \beta_j \left( 2 x_{ij} W_{jk} - W_{jk}^2 - 2 \sum_{l \neq k} W_{jk} W_{jl} \widehat{z_{il}} \right) \\
			&=& \sum_j \beta_j \left( x_{ij} W_{jk} - \frac{1}{2} W_{jk}^2 - \sum_{l \neq k} W_{jk} W_{jl} \widehat{z_{il}} \right) \\
			&=& \sum_j x_{ij} \beta_j W_{jk} - \frac{1}{2} \sum_j W_{jk} \beta_j W_{jk} - \nonumber \\
			&& \qquad \sum_j \beta_j \sum_{l \neq k} W_{jk} W_{jl} \widehat{z_{il}} \\
			&=& \sum_j x_{ij} \beta_j W_{jk} - \frac{1}{2} \sum_j W_{jk} \beta_j W_{jk} - \nonumber \\
			&& \qquad \sum_{l \neq k} \left( \sum_j W_{jl} \beta_j W_{jk} \right) \widehat{z_{il}} \\
			&=& \bm{x}_i^T \bm{\beta} \bm{W}_{:k} - \frac{1}{2} \bm{W}_{:k}^T \bm{\beta} \bm{W}_{:k} - \sum_{l \neq k} \bm{W}_{:l}^T \bm{\beta} \bm{W}_{:k} \widehat{z_{il}}
		\end{eqnarray}
		
		\item これより、$\mathcal{L}(q)$の$\widehat{z_{ik}}$による微分は次のようになる
		\begin{eqnarray}
			&& \frac{\partial}{\partial \widehat{z_{ik}}} \mathcal{L}(q) \nonumber \\
			&=& b_{ik} - \ln \widehat{z_{ik}} + \ln (1 - \widehat{z_{ik}}) + \nonumber \\
			&& \qquad \bm{x}_i^T \bm{\beta} \bm{W}_{:k} - \frac{1}{2} \bm{W}_{:k}^T \bm{\beta} \bm{W}_{:k} - \sum_{l \neq k} \bm{W}_{:l}^T \bm{\beta} \bm{W}_{:k} \widehat{z_{il}}
		\end{eqnarray}
		
		\item これを$0$と等置して、$\widehat{z_{ik}}$について解くと次のようになる
		\begin{eqnarray}
			&& \ln \widehat{z_{ik}} - \ln (1 - \widehat{z_{ik}}) = b_{ik} + \nonumber \\
			&& \qquad \bm{x}_i^T \bm{\beta} \bm{W}_{:k} - \frac{1}{2} \bm{W}_{:k}^T \bm{\beta} \bm{W}_{:k} - \sum_{l \neq k} \bm{W}_{:l}^T \bm{\beta} \bm{W}_{:k} \widehat{z_{il}}
		\end{eqnarray}
		右辺を$A$とおけば
		\begin{eqnarray}
			&& \ln \widehat{z_{ik}} - \ln (1 - \widehat{z_{ik}}) = A \\
			&\Rightarrow& \ln \frac{\widehat{z_{ik}}}{1 - \widehat{z_{ik}}} = A \nonumber \\
			&\Rightarrow& \frac{\widehat{z_{ik}}}{1 - \widehat{z_{ik}}} = \exp A \nonumber \\
			&\Rightarrow& \frac{1}{1 - \widehat{z_{ik}}} - 1 = \exp A \nonumber \\
			&\Rightarrow& 1 - \widehat{z_{ik}} = \frac{1}{1 + \exp A} \nonumber \\
			&\Rightarrow& \widehat{z_{ik}} = 1 - \frac{1}{1 + \exp A} \nonumber \\
			&\Rightarrow& \widehat{z_{ik}} = \frac{\exp A}{1 + \exp A} \nonumber \\
			&\Rightarrow& \widehat{z_{ik}} = \frac{1}{1 + \exp (-A)} \\
			&\Rightarrow& \widehat{z_{ik}} = \sigma(A)
		\end{eqnarray}
		従って、不動点方程式は
		\begin{equation}
			\widehat{z_{ik}} = \sigma \left( b_{ik} + \bm{x}_i^T \bm{\beta} \bm{W}_{:k} - \frac{1}{2} \bm{W}_{:k}^T \bm{\beta} \bm{W}_{:k} - \sum_{l \neq k} \bm{W}_{:l}^T \bm{\beta} \bm{W}_{:k} \widehat{z_{il}} \right)
		\end{equation}
	\end{itemize} \
	
	\item 不動点方程式の観察
	\begin{itemize}
		\item 不動点方程式は次で表された
		\begin{equation}
			\widehat{z_{ik}} = \sigma \left( b_{ik} + \bm{x}_i^T \bm{\beta} \bm{W}_{:k} - \frac{1}{2} \bm{W}_{:k}^T \bm{\beta} \bm{W}_{:k} - \sum_{l \neq k} \bm{W}_{:l}^T \bm{\beta} \bm{W}_{:k} \widehat{z_{il}} \right)
		\end{equation}
		
		\item 第2項$\bm{x}_i^T \bm{\beta} \bm{W}_{:k}$は、潜在変数のユニット$k$に対する入力
		\item 第3項$- \frac{1}{2} \bm{W}_{:k}^T \bm{\beta} \bm{W}_{:k}$は、隠れユニット$k$から自身への入力
		\item 第4項$- \sum_{l \neq k} \bm{W}_{:l}^T \bm{\beta} \bm{W}_{:k} \widehat{z_{il}}$は、他の隠れユニット$l \neq k$から隠れユニット$k$への入力
		\newline
		
		\item これより、平均場不動点方程式は、回帰結合型ニューラルネットワーク(RNN)との関係があることが分かる
		\newline
		
		\item 隠れユニット$k$と$l$は、それらの重みベクトル$\bm{W}_{:l}$と$\bm{W}_{:k}$が互いに同調するとき(\alert{似たような重みを持っているとき})に、\alert{互いに抑制し合う}
		\item 即ち、2つの隠れユニット$k, l$が、共に入力を説明するとき(入力から同じような表現を抽出するとき)、\alert{入力を最もよく説明するユニットのみがアクティブ}になる(強く活性化される)
		\item これはユニット間の競合の一形態である
		\newline
		
		\item 従って、実際には多峰性の事後分布かもしれないが、そのうちの1つのみが選択される(図\ref{fig:comparison-between-kl-divergence-2}の(b)と(c)参照)
		\newline
		
		\item 不動点方程式を更に以下のように変形する
		\begin{equation}
			\widehat{z_{ik}} = \sigma \left( b_{ik} + \left( \bm{x}_i - \sum_{l \neq k} \bm{W}_{:l} \widehat{z_{il}} \right)^T \bm{\beta} \bm{W}_{:k} - \frac{1}{2} \bm{W}_{:k}^T \bm{\beta} \bm{W}_{:k} \right)
		\end{equation}
		
		\item これより、ユニット$k$への入力は、$\bm{x}_i$ではなく$\bm{x}_i - \sum_{l \neq k} \bm{W}_{:l} \widehat{z_{il}}$であるとみなせる
		\item ユニット$k$への入力は、他の全てのユニットによる$\bm{x}_i$の再構成と、実際の入力$\bm{x}_i$との誤差である
		\item ユニット$k$は、この残差誤差を符号化していると分かるので、スパース符号化は、\alert{反復自己符号化器}とみなせる
		\newline
		
		\item スパース符号化では、入力$\bm{x}_i$の符号化($\widehat{z_{ik}}$の計算)と、復号($\sum_{l \neq k} \bm{W}_{:l} \widehat{z_{il}}$の計算)を繰り返す
		\item この反復のたびに、再構成の誤差を修正していく
	\end{itemize} \
	
	\item ダンピング
	\begin{itemize}
		\item 1つのユニットの更新則を(不動点方程式として)導出した
		\item 複数のユニットを同時に更新することは、二値スパース符号化モデルでは通常できない
		\item 但し、\alert{ダンピング}という発見的手法を使えば可能になる
		\newline
		
		\item 各要素$\widehat{z_{ik}}$についての最適値を計算し、その値の変化の方向に、他の要素$\widehat{z_{il}}$を小さいステップで動かす
		\item 下界$\mathcal{L}(q)$が増加することはもはや保証されないが、多くの場合はうまくいく
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{離散潜在変数の変分推論のまとめ}

\begin{itemize}
	\item ここまでの話の流れ
	\begin{enumerate}
		\item 離散潜在変数における変分推論の具体例として、二値スパース符号化モデルをみた
		\newline
		\item 事後分布$p(\bm{Z} | \bm{X})$が複雑になるので、最尤推定(EMアルゴリズム)が使えない
		\item 代わりに、別の分布$q(\bm{Z})$を使って事後分布を近似することにした
		\newline
		\item エビデンス下界$\mathcal{L}(q)$を\alert{苦労して求めた}
		\item 更に、下界$\mathcal{L}(q)$を(各パラメータについて)最大化するための、不動点方程式を導出した
		\newline
		\item 不動点方程式を観察し、回帰結合型ニューラルネットワークや、自己符号化器との関係を考えた
	\end{enumerate} \
	
	\item これからの話の流れ
	\begin{itemize}
		\item 連続潜在変数に対する変分推論を、簡単な確率モデルを使って、試してみよう
	\end{itemize}
\end{itemize}

\end{frame}

\subsection{連続潜在変数の変分推論}

\begin{frame}{連続潜在変数の変分推論}

\begin{itemize}
	\item 連続潜在変数をもつ変分推論の概要
	\begin{itemize}
		\item 平均場近似を行う場合は、以下の式によって最適な因子$q_j^*(\bm{Z}_j | \bm{X})$が得られる
		\begin{eqnarray}
			&& \ln q_j^*(\bm{Z}_j | \bm{X}) = \mathbb{E}_{i \neq j} \left[ \ln p(\bm{X}, \bm{Z}) \right] + \mathrm{Const.} \\
			&& \mathbb{E}_{i \neq j} \left[ \ln p(\bm{X}, \bm{Z}) \right] = \int \ln p(\bm{X}, \bm{Z}) \prod_{i \neq j} q_i(\bm{Z}_i | \bm{X}) d\bm{Z}_i \\
			&& q_j^*(\bm{Z}_j | \bm{X}) = \frac{\exp \left( \mathbb{E}_{i \neq j} \left[ \ln p(\bm{X}, \bm{Z}) \right] \right)}{\displaystyle \int \exp \left( \mathbb{E}_{i \neq j} \left[ \ln p(\bm{X}, \bm{Z}) \right] \right) d\bm{Z}_j}
		\end{eqnarray}
		
		\item 上式は、下界$\mathcal{L}(q)$を最大化する$q$であり、従って連続潜在変数の場合の\alert{不動点方程式}とみなせる
		\item 各因子$q_j(\bm{Z}_j | \bm{X})$を、上式を用いて順番に更新していく(\color{red}$i \neq j$である全ての$q_i$を固定\normalcolor した状態で、各$q_j$について下界を最適化する)
		\item このステップを、下界$\mathcal{L}(q)$が収束するまで繰り返し行う(座標降下法)
		\newline
		
		\item 不動点方程式は、下界が最適な値に\alert{収束するかどうかには関係なく}、\color{red}$q_j$の最適解が取る関数形を提供\normalcolor してくれる
	\end{itemize} \
	
	\item 扱う確率モデルの表現
	\begin{itemize}
		\item ここでは次のような確率モデルを対象として、変分推論を扱う
		\begin{eqnarray}
			&& p(\bm{z}_i) = \mathcal{N}(\bm{z}_i | 0, \bm{I}) \\
			&& p(x_i | \bm{z}_i) = \mathcal{N}(x_i | \bm{w}^T \bm{z}_i, 1)
		\end{eqnarray}
		
		\item \alert{1次元}のデータ$x \in \mathbb{R}$に対して、\alert{2次元}の潜在変数$\bm{z} \in \mathbb{R}^2$が存在する
		\item 同時分布$p(x_i, \bm{z}_i) = p(x_i | \bm{z}_i) p(\bm{z}_i)$を$\bm{z}_i$で積分消去すれば、$x_i$についての単なるガウス分布となる
	\end{itemize} \
	
	\item 真の事後分布$p(\bm{z}_i | x_i)$の計算
	\begin{itemize}
		\item 正規化定数を無視して次のように計算できる
		\begin{eqnarray}
			&& p(\bm{z}_i | x_i) \nonumber \\
			&\propto& p(\bm{z}_i | x_i) p(x_i) \nonumber \\
			&=& p(x_i, \bm{z}_i) \nonumber \\
			&=& p(\bm{z}_i) p(x_i | \bm{z}_i) \nonumber \\
			&=& \mathcal{N}(\bm{z}_i | 0, \bm{I}) \mathcal{N}(x_i | \bm{w}^T \bm{z}_i, 1) \nonumber \\
			&\propto& \exp \left( -\frac{1}{2} \bm{z}_i^T \bm{z}_i \right) \exp \left( -\frac{1}{2} \left( x_i - \bm{w}^T \bm{z}_i \right)^T \left( x_i - \bm{w}^T \bm{z}_i \right) \right) \nonumber \\
			&=& \exp \left( -\frac{1}{2} \left( z_{i1}^2 + z_{i2}^2 \right) \right) \nonumber \\
			&& \qquad \exp \left( -\frac{1}{2} \left( x_i - w_1 z_{i1} - w_2 z_{i2} \right)^T \left( x_i - w_1 z_{i1} - w_2 z_{i2} \right) \right) \nonumber \\
			&=& \exp \left( -\frac{1}{2} \left( z_{i1}^2 + z_{i2}^2 + x_i^2 + w_1^2 z_{i1}^2 + w_2^2 z_{i2}^2 - \right. \right. \nonumber \\
			&& \qquad 2 x_i w_1 z_{i1} - 2 x_i w_2 z_{i2} + 2 w_1 w_2 z_{i1} z_{i2} \big) \bigg)
		\end{eqnarray}
		
		\item 正規化項を$C$とすれば次のように書ける
		\begin{eqnarray}
			&& p(\bm{z}_i | x_i) \nonumber \\
			&=& C \exp \left( -\frac{1}{2} \left( z_{i1}^2 + z_{i2}^2 + x_i^2 + w_1^2 z_{i1}^2 + w_2^2 z_{i2}^2 - \right. \right. \nonumber \\
			&& \qquad 2 x_i w_1 z_{i1} - 2 x_i w_2 z_{i2} + 2 w_1 w_2 z_{i1} z_{i2} \big) \bigg)
		\end{eqnarray}
		
		\item $z_{i1}$と$z_{i2}$を乗算する項が存在する
		\item 従って、真の事後分布は、\color{red}$z_{i1}$と$z_{i2}$のみの因子には分解できない\normalcolor ことが分かる
	\end{itemize} \
	
	\item 平均場近似の計算
	\begin{itemize}
		\item 平均場近似を次のように表現する
		\begin{equation}
			q(\bm{z}_i | x_i) = q_1(z_{i1} | x_i) q_2(z_{i2} | x_i)
		\end{equation}
		
		\item $q_2$を固定した状態で、最適な$q_1^*(z_{i1} | x_i)$を求める
		\begin{eqnarray}
			&& \ln q_1^*(z_{i1} | x_i) \nonumber \\
			&=& \mathbb{E}_{z_{i2} \sim q_2(z_{i2} | x_i)} \left[ \ln p(\bm{z}_i, x_i) \right] + \mathrm{Const.} \\
			&=& \mathbb{E}_{z_{i2}} \left[ \ln \left( C \exp \left( -\frac{1}{2} \left( z_{i1}^2 + z_{i2}^2 + x_i^2 + w_1^2 z_{i1}^2 + w_2^2 z_{i2}^2 - \right. \right. \right. \right. \nonumber \\
			&& \qquad 2 x_i w_1 z_{i1} - 2 x_i w_2 z_{i2} + 2 w_1 w_2 z_{i1} z_{i2} \big) \bigg) \bigg) \bigg] + \mathrm{Const.} \nonumber \\
			&=& \mathbb{E}_{z_{i2}} \left[ -\frac{1}{2} \left( z_{i1}^2 + z_{i2}^2 + x_i^2 + w_1^2 z_{i1}^2 + w_2^2 z_{i2}^2 - \right. \right. \nonumber \\
			&& \qquad 2 x_i w_1 z_{i1} - 2 x_i w_2 z_{i2} + 2 w_1 w_2 z_{i1} z_{i2} \big) \bigg] + \mathrm{Const.}
		\end{eqnarray}
		
		\item $z_{i2} \sim q_2(z_{i2} | x_i)$による期待値を取っている
		\item $q_2(z_{i2} | x_i)$から得る必要があるのは、結局$\mathbb{E}_{z_{i2}}[z_{i2}]$と、$\mathbb{E}_{z_{i2}}[z_{i2}^2]$の2つだけである
		\item $\left< z_{i2} \right> = \mathbb{E}_{z_{i2}}[z_{i2}]$、$\left< z_{i2}^2 \right> = \mathbb{E}_{z_{i2}}[z_{i2}^2]$と書くことにする
		\newline
		
		\item このとき次式が得られる
		\begin{eqnarray}
			&& \ln q_1^*(z_{i1} | x_i) \nonumber \\
			&=& -\frac{1}{2} \left( z_{i1}^2 + \left< z_{i2}^2 \right> + x_i^2 + w_1^2 z_{i1}^2 + w_2^2 \left< z_{i2}^2 \right> - \right. \nonumber \\
			&& \qquad \left. 2 x_i w_1 z_{i1} - 2 x_i w_2 \left< z_{i2} \right> + 2 w_1 w_2 z_{i1} \left< z_{i2} \right> \right) + \nonumber \\
			&& \qquad \mathrm{Const.}
		\end{eqnarray}
		
		\item これより、最適な$q_1^*(z_{i1} | x_i)$は\alert{ガウス分布}の形であると分かる
		\begin{eqnarray}
			&& q_1^*(z_{i1} | x_i) \nonumber \\
			&=& C \exp \left( -\frac{1}{2} \left( z_{i1}^2 + \left< z_{i2}^2 \right> + x_i^2 + w_1^2 z_{i1}^2 + w_2^2 \left< z_{i2}^2 \right> - \right. \right. \nonumber \\
			&& \qquad 2 x_i w_1 z_{i1} - 2 x_i w_2 \left< z_{i2} \right> + 2 w_1 w_2 z_{i1} \left< z_{i2} \right> \big) \bigg) \nonumber \\
			&\propto& \exp \left( -\frac{1}{2} \left( z_{i1}^2 + \left< z_{i2}^2 \right> + x_i^2 + w_1^2 z_{i1}^2 + w_2^2 \left< z_{i2}^2 \right> - \right. \right. \nonumber \\
			&& \qquad 2 x_i w_1 z_{i1} - 2 x_i w_2 \left< z_{i2} \right> + 2 w_1 w_2 z_{i1} \left< z_{i2} \right> \big) \bigg)
		\end{eqnarray}
		
		\item 対称性から、最適な$q_2^*(z_{i2} | x_i)$も\alert{ガウス分布}であることが分かる
		\newline
		
		\item ガウス分布同士の積もガウス分布になるので、結局$q(\bm{z}_i | x_i) = q_1(z_{i1} | x_i) q_2(z_{i2} | x_i)$はガウス分布である
		\item $q(\bm{z}_i | x_i)$が2つの因子に分解できるとは仮定したが、\alert{各因子の関数形については全く仮定していない}ことに注意
		\item ガウス分布は、下界$\mathcal{L}(q)$を$q$について変分最適化する過程で、\alert{自然に出現した}
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{連続潜在変数の変分推論のまとめ}

\begin{itemize}
	\item ここまでの話の流れ
	\begin{enumerate}
		\item 連続潜在変数における変分推論の例として、簡単な確率モデルを扱った
		\newline
		\item 分布$q(\bm{Z} | \bm{X})$を、$\prod_i q_i(\bm{Z}_i | \bm{X})$のように因数分解できるという仮定(\alert{平均場近似})のみを置いた
		\item 各因子$q_i(\bm{Z}_i | \bm{X})$の関数形については全く仮定を置かなかった
		\newline
		\item 変分推論によって、最適解となる$q_i$の関数形が自然に導出できた
	\end{enumerate} \
	
	\item これからの話の流れ
	\begin{itemize}
		\item 近似推論が、推論アルゴリズムの精度に影響することについて考える
	\end{itemize}
\end{itemize}

\end{frame}

% 下界とは何かという説明
% エントロピーを最大化する分布がガウス分布であることの説明
% 下界を2つに分解したときの、KL距離が正則化項であることの説明
% Variational Auto Encoder (VAE)の説明

\end{document}
